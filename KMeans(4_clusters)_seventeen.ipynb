{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "KMeans(4_clusters)_seventeen.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "1b7668YbgrxR992-za21rJQ3OArSIXg8Q",
      "authorship_tag": "ABX9TyOHqb6iKmvcrUJtNXJ1yZwT",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/IDF13/mulcam_army/blob/sumin/KMeans(4_clusters)_seventeen.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GIzEXB_CkWsX",
        "outputId": "fb5d84cb-e979-4fe4-f38f-92cc756aac50"
      },
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "\"\"\"k-means_code.ipynb\n",
        "\n",
        "Automatically generated by Colaboratory.\n",
        "\n",
        "Original file is located at\n",
        "    https://colab.research.google.com/drive/1Ls1XFeatzdMivoFijtX0MuN9E7ADviAC\n",
        "\"\"\"\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.datasets import make_blobs\n",
        "\n",
        "\"\"\"# ë°ì´í„° ë¶ˆëŸ¬ì˜¤ê¸°\n",
        "ëŒ“ê¸€ì„ ì–¼ë§ˆë‚˜ ë¶ˆëŸ¬ì™€ì•¼ í• ê¹Œ\n",
        "\"\"\"\n",
        "\n",
        "# ë°ì´í„° ë¶ˆëŸ¬ì˜¤ê¸°\n",
        "\n",
        "path = '/content/drive/MyDrive/[á„€á…©á†¼á„‹á…²] Mulcam_Army á„€á…©á†¼á„‹á…²á„‘á…©á†¯á„ƒá…¥!/á„á…³á„…á…©á†¯á„…á…µá†¼ á„’á…¡á†« á„Œá…¡á„…á…­/k-pop_Radará„‹á…¡á„á…µá„‰á…³á„á…³ á„á…³á„…á…©á†¯á„…á…µá†¼ á„’á…¡á†¯á„ƒá…¡á†¼á„‡á…®á†«/á„‰á…®á„†á…µá†«_á„€á…§á†¯á„€á…ªá„†á…®á†¯/'\n",
        "comment_file = 'prepro_stats_page_640á„‰á…¦á„‡á…³á†«á„á…µá†«.csv' \n",
        "\n",
        "data = pd.read_csv(path+comment_file, encoding='utf-8', header=None)\n",
        "data.columns = ['comment','like','lang']\n",
        "print(len(data))\n",
        "data.head()\n",
        "\n",
        "data_ko = pd.DataFrame([kor[:1] for kor in data.values if kor[2] == '(ko)'], columns=['comment'])\n",
        "data_en = pd.DataFrame([en[:1] for en in data.values if en[2] == '(en)'], columns=['comment'])\n",
        "data_en.comment.values\n",
        "\n",
        "import nltk\n",
        "from nltk.corpus import stopwords \n",
        "from nltk.tokenize import word_tokenize\n",
        "import re\n",
        "\n",
        "for i in range(len(data_en.comment)):\n",
        "    data_en.comment[i] = str(data_en.comment[i])\n",
        "\n",
        "# ìˆ«ìì œê±° / ë°‘ì¤„ ì œì™¸í•œ íŠ¹ìˆ˜ë¬¸ì ì œê±°\n",
        "p = re.compile(\"[0-7]+\")\n",
        "z = re.compile(\"[8-9]+\")\n",
        "q = re.compile(\"\\W+\")\n",
        "r = re.compile('[^a-zA-Z]+')\n",
        "\n",
        "en = []\n",
        "for i in data_en.comment.values:\n",
        "    tokens = re.sub(p,\" \",i)\n",
        "    tokens = re.sub(z,\" \",tokens)\n",
        "    tokens = re.sub(q,\" \",tokens)\n",
        "    tokens = re.sub(r,\" \", tokens)\n",
        "    en.append(tokens)\n",
        "len(en)\n",
        "en[:2]\n",
        "\n",
        "# ë¶ˆìš©ì–´ ì œê±°\n",
        "import nltk\n",
        "from nltk.corpus import stopwords \n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "\n",
        "stop_words = set(stopwords.words('english')) \n",
        "# stop_words.update(('song','group','songs','youtube','views','time','https','girl','girls','people','yes','lol','video','part','member','members', 'look','way','guys','fans','fan'))\n",
        "\n",
        "res=[]\n",
        "for i in range(len(en)):\n",
        "    word_tokens = word_tokenize(en[i])\n",
        "\n",
        "    result = []\n",
        "    for w in word_tokens: \n",
        "        if w not in stop_words: \n",
        "            result.append(w) \n",
        "    res.append(result)\n",
        "\n",
        "# print(word_tokens) \n",
        "print(res[:10])\n",
        "print(len(res))\n",
        "\n",
        "en_pos = []\n",
        "for i in range(len(res)):\n",
        "    tokens_pos = nltk.pos_tag(res[i])\n",
        "    en_pos.append(tokens_pos)\n",
        "\n",
        "en_pos[:5]\n",
        "\n",
        "# ëª…ì‚¬ëŠ” NNì„ í¬í•¨í•˜ê³  ìˆìŒì„ ì•Œ ìˆ˜ ìˆìŒ\n",
        "en_NN=[]\n",
        "for i in range(len(en_pos)):\n",
        "    NN_words = []\n",
        "    for word, pos in en_pos[i]:\n",
        "        if 'NN' in pos:\n",
        "            NN_words.append(word)\n",
        "        elif 'NN' in pos:\n",
        "            NN_words.append(word)\n",
        "    en_NN.extend(NN_words)\n",
        "en_NN[:10]\n",
        "\n",
        "# df = pd.DataFrame(en_NN)\n"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "83238\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
            "[['even', 'carat', 'let', 'admit', 'song', 'universal'], ['song', 'tells', 'song', 'need', 'billion', 'views', 'prove', 'legendary'], ['fan', 'say', 'song', 'another', 'level'], ['years', 'passed', 'song', 'still', 'legend', 'probably', 'legacy', 'passed', 'different', 'generation', 'generation', 'attacca', 'released', 'october', 'hope', 'able', 'reach', 'new', 'milestone', 'boys', 'lately', 'previous', 'comebacks', 'struggle', 'make', 'views', 'go', 'together', 'almost', 'years', 'ripe', 'already', 'still', 'lack', 'effort', 'whilst', 'seventeen', 'work', 'hard', 'satisfy', 'us', 'every', 'menu', 'aim', 'make', 'us', 'taste', 'unending', 'gose', 'segments', 'contents', 'hope', 'able', 'pay', 'back'], ['fan', 'really', 'like', 'songs', 'especially', 'one', 'deserves', 'attention', 'views'], ['funfact', 'everyone', 'agree', 'successful', 'comeback', 'seventeen'], ['proof', 'choreography', 'need', 'jumps', 'flips', 'beautifully', 'done', 'goes', 'well', 'music', 'probably', 'one', 'favorites', 'also', 'video', 'choreography', 'danced', 'x', 'speed', 'left', 'awe', 'well', 'synchronized'], ['forget', 'fandom', 'carat', 'comes', 'song'], ['proves', 'songs', 'need', 'badass', 'become', 'addictive'], ['seventeen', 'stan', 'like', 'guys', 'deserves', 'views', 'likes', 'masterpiece']]\n",
            "69835\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['admit',\n",
              " 'song',\n",
              " 'universal',\n",
              " 'song',\n",
              " 'tells',\n",
              " 'views',\n",
              " 'fan',\n",
              " 'level',\n",
              " 'years',\n",
              " 'song']"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fsPhE4L_6UjY",
        "outputId": "9b00d6a5-4efc-4331-e01b-704078818db0"
      },
      "source": [
        "## 3ë‹¨ì–´ ì´í•˜ ì§§ì€ ë‹¨ì–´ ì œê±°\n",
        " # remove words less than three letters\n",
        "# print(res[1])\n",
        "# for word in res[1]:\n",
        "#     print(word)\n",
        "en_sent_less3=[]\n",
        "for i in range(len(res)):\n",
        "    tokens = [word for word in res[i] if len(word) >= 3]\n",
        "    en_sent_less3.append(tokens)\n",
        "en_sent_less3[:2]"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[['even', 'carat', 'let', 'admit', 'song', 'universal'],\n",
              " ['song', 'tells', 'song', 'need', 'billion', 'views', 'prove', 'legendary']]"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KQjt3WNR6dFW",
        "outputId": "3f929e7e-7491-4283-d5c1-0c976d9d02fc"
      },
      "source": [
        "en_sent =[]\n",
        "for i in range(len(en_sent_less3)):\n",
        "    temp=\" \".join(en_sent_less3[i])\n",
        "    en_sent.append(temp)\n",
        "en_sent[:15]"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['even carat let admit song universal',\n",
              " 'song tells song need billion views prove legendary',\n",
              " 'fan say song another level',\n",
              " 'years passed song still legend probably legacy passed different generation generation attacca released october hope able reach new milestone boys lately previous comebacks struggle make views together almost years ripe already still lack effort whilst seventeen work hard satisfy every menu aim make taste unending gose segments contents hope able pay back',\n",
              " 'fan really like songs especially one deserves attention views',\n",
              " 'funfact everyone agree successful comeback seventeen',\n",
              " 'proof choreography need jumps flips beautifully done goes well music probably one favorites also video choreography danced speed left awe well synchronized',\n",
              " 'forget fandom carat comes song',\n",
              " 'proves songs need badass become addictive',\n",
              " 'seventeen stan like guys deserves views likes masterpiece',\n",
              " 'today wan cry year anniversary thank seventeen give masterpiece song',\n",
              " 'remember watching way back release stanning svt really like hoshi without knowing deeper',\n",
              " 'seventeen song recommendations non fans home lean listen secret highlight change heaven cloud fast pace habit trauma good meplease give discography chance seventeen lot great songs',\n",
              " 'suppose cry title say really stop crying time find masterpiece comforting',\n",
              " 'nobody crys cuz crush love wan cry']"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q30F-7Ii6mhg"
      },
      "source": [
        "data_en['en_sent']=en_sent"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "id": "b5R_g5Zn6oMF",
        "outputId": "1a9d96a7-99f5-44ea-9185-922776ec3669"
      },
      "source": [
        "data_en.tail()"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>comment</th>\n",
              "      <th>en_sent</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>69830</th>\n",
              "      <td>im updated but i want to binge watch more svt ...</td>\n",
              "      <td>updated want binge watch svt videos</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>69831</th>\n",
              "      <td>anyways im currently binge watching their vliv...</td>\n",
              "      <td>anyways currently binge watching vlives help</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>69832</th>\n",
              "      <td>novesuity dino thank you tho ğŸ¥°</td>\n",
              "      <td>novesuity dino thank tho</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>69833</th>\n",
              "      <td>lui quinto did you watch seventeen project big...</td>\n",
              "      <td>lui quinto watch seventeen project big debut a...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>69834</th>\n",
              "      <td>ë§‰ë‚´ì˜¨íƒ‘ seventeen im currently watching those</td>\n",
              "      <td>seventeen currently watching</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                 comment                                            en_sent\n",
              "69830  im updated but i want to binge watch more svt ...                updated want binge watch svt videos\n",
              "69831  anyways im currently binge watching their vliv...       anyways currently binge watching vlives help\n",
              "69832                     novesuity dino thank you tho ğŸ¥°                           novesuity dino thank tho\n",
              "69833  lui quinto did you watch seventeen project big...  lui quinto watch seventeen project big debut a...\n",
              "69834         ë§‰ë‚´ì˜¨íƒ‘ seventeen im currently watching those                       seventeen currently watching"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 419
        },
        "id": "KqeGJs2JktNt",
        "outputId": "e3506f07-f2e4-4014-c2c1-92cde58c2536"
      },
      "source": [
        "# TF_IDF ë²¡í„°í™”\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics import adjusted_rand_score\n",
        "from sklearn.cluster import KMeans\n",
        "\n",
        "docs = data_en\n",
        "# len(docs)\n",
        "docs"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>comment</th>\n",
              "      <th>en_sent</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>even if you re not a carat let s all admit it ...</td>\n",
              "      <td>even carat let admit song universal</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>this song tells that some song s don t need bi...</td>\n",
              "      <td>song tells song need billion views prove legen...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>not a fan but i should say that this song is o...</td>\n",
              "      <td>fan say song another level</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4 years have passed and this song is still a l...</td>\n",
              "      <td>years passed song still legend probably legacy...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>i m not a fan of them but i really like all of...</td>\n",
              "      <td>fan really like songs especially one deserves ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>69830</th>\n",
              "      <td>im updated but i want to binge watch more svt ...</td>\n",
              "      <td>updated want binge watch svt videos</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>69831</th>\n",
              "      <td>anyways im currently binge watching their vliv...</td>\n",
              "      <td>anyways currently binge watching vlives help</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>69832</th>\n",
              "      <td>novesuity dino thank you tho ğŸ¥°</td>\n",
              "      <td>novesuity dino thank tho</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>69833</th>\n",
              "      <td>lui quinto did you watch seventeen project big...</td>\n",
              "      <td>lui quinto watch seventeen project big debut a...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>69834</th>\n",
              "      <td>ë§‰ë‚´ì˜¨íƒ‘ seventeen im currently watching those</td>\n",
              "      <td>seventeen currently watching</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>69835 rows Ã— 2 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                 comment                                            en_sent\n",
              "0      even if you re not a carat let s all admit it ...                even carat let admit song universal\n",
              "1      this song tells that some song s don t need bi...  song tells song need billion views prove legen...\n",
              "2      not a fan but i should say that this song is o...                         fan say song another level\n",
              "3      4 years have passed and this song is still a l...  years passed song still legend probably legacy...\n",
              "4      i m not a fan of them but i really like all of...  fan really like songs especially one deserves ...\n",
              "...                                                  ...                                                ...\n",
              "69830  im updated but i want to binge watch more svt ...                updated want binge watch svt videos\n",
              "69831  anyways im currently binge watching their vliv...       anyways currently binge watching vlives help\n",
              "69832                     novesuity dino thank you tho ğŸ¥°                           novesuity dino thank tho\n",
              "69833  lui quinto did you watch seventeen project big...  lui quinto watch seventeen project big debut a...\n",
              "69834         ë§‰ë‚´ì˜¨íƒ‘ seventeen im currently watching those                       seventeen currently watching\n",
              "\n",
              "[69835 rows x 2 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w9QzRrGv7OOr",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 244
        },
        "outputId": "3a886b78-1501-456e-e7f8-1d68c2664ab0"
      },
      "source": [
        "tfidf = TfidfVectorizer(stop_words = 'english', \n",
        "                        #min_df = 3,  # 3íšŒ ë¯¸ë§Œìœ¼ë¡œ ë“±ì¥í•˜ëŠ” í† í°ì€ ë¬´ì‹œ\n",
        "                        max_df = 0.95 # ë§ì´ ë“±ì¥í•œ ë‹¨ì–´ 5%ì˜ í† í°ë„ ë¬´ì‹œ\n",
        "                        )\n",
        "docs_tf = tfidf.fit_transform(docs.comment.values)\n",
        "\n",
        "# (stop_words='english')\n",
        "# token_pattern='(?u)\\\\b\\\\w+\\\\b' or 't\\w+'\n",
        "# ngram_range : ë‹¨ì–´ì¥ ìƒì„±ì— í•„ìš”í•œ í† í°ì˜ í¬ê¸°       \n",
        "# listì¼ ê²½ìš° : fit['']\n",
        "# https://wikidocs.net/33661 -> tf-idf ë§¤ê°œë³€ìˆ˜"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-8-4f60356f5877>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m tfidf = TfidfVectorizer(stop_words = 'english', \n\u001b[0m\u001b[1;32m      2\u001b[0m                         \u001b[0;31m#min_df = 3,  # 3íšŒ ë¯¸ë§Œìœ¼ë¡œ ë“±ì¥í•˜ëŠ” í† í°ì€ ë¬´ì‹œ\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m                         \u001b[0mmax_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.95\u001b[0m \u001b[0;31m# ë§ì´ ë“±ì¥í•œ ë‹¨ì–´ 5%ì˜ í† í°ë„ ë¬´ì‹œ\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m                         )\n\u001b[1;32m      5\u001b[0m \u001b[0mdocs_tf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtfidf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdocs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcomment\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'TfidfVectorizer' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4DXO-_yasx84",
        "outputId": "f131ffed-e032-4d52-ebb6-76a93826df2d"
      },
      "source": [
        "docs.shape"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(69835, 2)"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jtXKktlO-O3e",
        "outputId": "2ecfd237-b8b8-4901-8219-d59ca120f0f0"
      },
      "source": [
        "docs_tf.shape"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(69835, 47677)"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ApeAyfIvb6mD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3a9476f7-f5eb-42fc-ee27-87458ceb6ddf"
      },
      "source": [
        "from sklearn.cluster import KMeans\n",
        "\n",
        "modelkmeans = KMeans(n_clusters =4, init='k-means++', n_init=300)\n",
        "modelkmeans.fit(docs_tf)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "KMeans(algorithm='auto', copy_x=True, init='k-means++', max_iter=300,\n",
              "       n_clusters=4, n_init=300, n_jobs=None, precompute_distances='auto',\n",
              "       random_state=None, tol=0.0001, verbose=0)"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ow7CgSynZpgR"
      },
      "source": [
        "from sklearn.cluster import KMeans\n",
        "\n",
        "inertia_arr = []\n",
        "k_range = range(1,11)\n",
        "\n",
        "for k in k_range :\n",
        "\n",
        "  km = KMeans(n_clusters=4, random_state=200)\n",
        "  km.fit(docs_tf)\n",
        "  interia = km.inertia_\n",
        "\n",
        "  print('k :', k, 'interia :', interia)\n",
        "\n",
        "  inertia_arr.append(interia)\n",
        "\n",
        "inertia_arr1 = np.array(inertia_arr)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ewc3d27cZuGf"
      },
      "source": [
        "plt.plot(k_range, inertia_arr, marker='o')\n",
        "plt.vlines(3, ymin=inertia_arr.min()*0.9999, ymax=inertia_arr.max()*1.0003, linestyles='--', colors = 'g')\n",
        "plt.vlines(4, ymin=inertia_arr.min()*0.9999, ymax=inertia_arr.max()*1.0003, linestyles='--', colors = 'r')\n",
        "\n",
        "plt.title('Elbow Method')\n",
        "plt.xlabel('Number of clusters')\n",
        "plt.ylabel('Inertia')\n",
        "plt.show()\n",
        "# k=3"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1s51Op27OLss"
      },
      "source": [
        "# êµ°ì§‘í™”í•œ ë ˆì´ë¸”ê°’ë“¤ì„ document_df ì— ì¶”ê°€í•˜ê¸°\n",
        "\n",
        "document_df['cluster_label'] = cluster_label\n",
        "print(document_df.sort_values(by=['cluster_label']))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jWtSRk3o9gA8"
      },
      "source": [
        "cluster_centers =  modelkmeans.cluster_centers_"
      ],
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kbMlrAY5OYSr",
        "outputId": "0c4c4978-73bd-4e87-911a-337882abd101"
      },
      "source": [
        "print(cluster_centers.shape)\n",
        "print(cluster_centers)"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(4, 47677)\n",
            "[[2.34696254e-03 3.71346633e-05 9.10290789e-06 ... 0.00000000e+00\n",
            "  1.79169727e-05 1.26692129e-05]\n",
            " [2.49781210e-04 0.00000000e+00 0.00000000e+00 ... 5.89872641e-05\n",
            "  0.00000000e+00 0.00000000e+00]\n",
            " [0.00000000e+00 0.00000000e+00 0.00000000e+00 ... 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00]\n",
            " [0.00000000e+00 0.00000000e+00 0.00000000e+00 ... 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8JiwFfFKCF1y",
        "outputId": "8196fc26-fdea-4baa-a2ca-6361f9753830"
      },
      "source": [
        "np.unique(modelkmeans.labels_)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0, 1, 2, 3], dtype=int32)"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fIruZ2WFEjKW"
      },
      "source": [
        "cluster_label = modelkmeans.labels_"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BLweiMk2E1Li"
      },
      "source": [
        "docs['cluster_label'] =  modelkmeans.labels_"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 419
        },
        "id": "FtI5G3qyFA4I",
        "outputId": "4607a7be-0a2b-436b-fc3b-c5d63975b685"
      },
      "source": [
        "docs"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>comment</th>\n",
              "      <th>en_sent</th>\n",
              "      <th>cluster_label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>even if you re not a carat let s all admit it ...</td>\n",
              "      <td>even carat let admit song universal</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>this song tells that some song s don t need bi...</td>\n",
              "      <td>song tells song need billion views prove legen...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>not a fan but i should say that this song is o...</td>\n",
              "      <td>fan say song another level</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4 years have passed and this song is still a l...</td>\n",
              "      <td>years passed song still legend probably legacy...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>i m not a fan of them but i really like all of...</td>\n",
              "      <td>fan really like songs especially one deserves ...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>69830</th>\n",
              "      <td>im updated but i want to binge watch more svt ...</td>\n",
              "      <td>updated want binge watch svt videos</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>69831</th>\n",
              "      <td>anyways im currently binge watching their vliv...</td>\n",
              "      <td>anyways currently binge watching vlives help</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>69832</th>\n",
              "      <td>novesuity dino thank you tho ğŸ¥°</td>\n",
              "      <td>novesuity dino thank tho</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>69833</th>\n",
              "      <td>lui quinto did you watch seventeen project big...</td>\n",
              "      <td>lui quinto watch seventeen project big debut a...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>69834</th>\n",
              "      <td>ë§‰ë‚´ì˜¨íƒ‘ seventeen im currently watching those</td>\n",
              "      <td>seventeen currently watching</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>69835 rows Ã— 3 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                 comment  ... cluster_label\n",
              "0      even if you re not a carat let s all admit it ...  ...             1\n",
              "1      this song tells that some song s don t need bi...  ...             1\n",
              "2      not a fan but i should say that this song is o...  ...             1\n",
              "3      4 years have passed and this song is still a l...  ...             0\n",
              "4      i m not a fan of them but i really like all of...  ...             0\n",
              "...                                                  ...  ...           ...\n",
              "69830  im updated but i want to binge watch more svt ...  ...             0\n",
              "69831  anyways im currently binge watching their vliv...  ...             0\n",
              "69832                     novesuity dino thank you tho ğŸ¥°  ...             3\n",
              "69833  lui quinto did you watch seventeen project big...  ...             0\n",
              "69834         ë§‰ë‚´ì˜¨íƒ‘ seventeen im currently watching those  ...             1\n",
              "\n",
              "[69835 rows x 3 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L9nbZ5LiX2RB"
      },
      "source": [
        "# í´ëŸ¬ìŠ¤í„°ë“¤ì˜ í•µì‹¬ë‹¨ì–´ ì¶”ì¶œ\n",
        "# https://techblog-history-younghunjo1.tistory.com/114\n",
        "\n",
        "def get_cluster_details(cluster_model, cluster_data, feature_names,\n",
        "                       cluster_num, top_n_features=10):\n",
        "    cluster_details = {}\n",
        "    # ê° í´ëŸ¬ìŠ¤í„° ë ˆì´ë¸”ë³„ featureë“¤ì˜ centerê°’ë“¤ ë‚´ë¦¼ì°¨ìˆœìœ¼ë¡œ ì •ë ¬ í›„ì˜ ì¸ë±ìŠ¤ë¥¼ ë°˜í™˜\n",
        "    center_feature_idx = cluster_model.cluster_centers_.argsort()[:,::-1]\n",
        "    \n",
        "    # ê°œë³„ í´ëŸ¬ìŠ¤í„° ë ˆì´ë¸”ë³„ë¡œ \n",
        "    for cluster_num in range(cluster_num):\n",
        "        # ê°œë³„ í´ëŸ¬ìŠ¤í„°ë³„ ì •ë³´ë¥¼ ë‹´ì„ empty dictí• ë‹¹\n",
        "        cluster_details[cluster_num] = {}\n",
        "        cluster_details[cluster_num]['cluster'] = cluster_num\n",
        "        \n",
        "        # ê° featureë³„ centerê°’ë“¤ ì •ë ¬í•œ ì¸ë±ìŠ¤ ì¤‘ ìƒìœ„ 10ê°œë§Œ ì¶”ì¶œ\n",
        "        top_ftr_idx = center_feature_idx[cluster_num, :top_n_features]\n",
        "        top_ftr = [feature_names[idx] for idx in top_ftr_idx]\n",
        "        # top_ftr_idxë¥¼ í™œìš©í•´ì„œ ìƒìœ„ 10ê°œ featureë“¤ì˜ centerê°’ë“¤ ë°˜í™˜\n",
        "        # ë°˜í™˜í•˜ê²Œ ë˜ë©´ arrayì´ê¸° ë–„ë¬¸ì— ë¦¬ìŠ¤íŠ¸ë¡œë°”ê¾¸ê¸°\n",
        "        top_ftr_val = cluster_model.cluster_centers_[cluster_num, top_ftr_idx].tolist()\n",
        "        \n",
        "        # cluster_details ë”•ì…”ë„ˆë¦¬ì—ë‹¤ê°€ ê°œë³„ êµ°ì§‘ ì •ë³´ ë„£ì–´ì£¼ê¸°\n",
        "        cluster_details[cluster_num]['top_features'] = top_ftr\n",
        "        cluster_details[cluster_num]['top_featrues_value'] = top_ftr_val\n",
        "\n",
        "        # í•´ë‹¹ cluster_numìœ¼ë¡œ ë¶„ë¥˜ëœ íŒŒì¼ëª…(ë¬¸ì„œë“¤) ë„£ì–´ì£¼ê¸°\n",
        "        doc_cl = cluster_data[cluster_data['cluster_label']==cluster_num]['']\n",
        "\n",
        "        # filenamesê°€ dfìœ¼ë¡œ ë°˜í™˜ë˜ê¸° ë–„ë¬¸ì— ê°’ë“¤ë§Œ ì¶œë ¥í•´ì„œ array->listë¡œ ë³€í™˜\n",
        "        filenames = doc_cl.values.tolist()\n",
        "        cluster_details[cluster_num][''] = filenames\n",
        "    \n",
        "    return cluster_details"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PEh2vklsX7Bb"
      },
      "source": [
        "def print_cluster_details(cluster_details):\n",
        "    for cluster_num, cluster_detail in cluster_details.items():\n",
        "        print(f\"#####Cluster Num: {cluster_num}\")\n",
        "        print()\n",
        "        print(\"ìƒìœ„ 10ê°œ featureë‹¨ì–´ë“¤:\\n\", cluster_detail['top_features'])\n",
        "        print()\n",
        "        print(f\"Cluster {cluster_num}ìœ¼ë¡œ ë¶„ë¥˜ëœ ë¬¸ì„œë“¤:\\n{cluster_detail['comment'][:5]}\")\n",
        "        print('-'*20)\n",
        "\n",
        "feature_names = tfidf.get_feature_names()\n",
        "cluster_details = get_cluster_details(cluster_model=km,\n",
        "                                     cluster_data=comment.values,\n",
        "                                     feature_names=feature_names,\n",
        "                                     cluster_num=3,\n",
        "                                     top_n_features=10)\n",
        "print_cluster_details(cluster_details)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o9tp_D_lXyJf"
      },
      "source": [
        " # ë¹ˆë„ë¶„ì„"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        },
        "id": "11SB0FCMQQxb",
        "outputId": "7ccf665c-9ce1-42cc-bbe0-34d4907deec5"
      },
      "source": [
        "from collections import Counter \n",
        "\n",
        "def scan_vocabulary(sents, tokenize, min_conunt=2): \n",
        "\n",
        "  counter = Counter(w for sent in sents for w in tokenize(sent)) \n",
        "  counter = {w: c for w, c in counter.items() if c >= min_count} \n",
        "  idx_to_vocab = [w for w, _ in sorted(counter.items(), key:lambda x:-x[1])] \n",
        "  vocab_to_idx = {vocab:idx for idx, vocab in enumerate(idx_to_vocab)} \n",
        "  \n",
        "return idx_to_vocab, vocab_to_idx\n",
        "\n",
        "# ì¶œì²˜: https://ebbnflow.tistory.com/292 [Dev Log : ì‚¶ì€ í™•ë¥ ì˜ êµ¬ë¦„]"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-38-6c1b8768c56d>\"\u001b[0;36m, line \u001b[0;32m7\u001b[0m\n\u001b[0;31m    idx_to_vocab = [w for w, _ in sorted(counter.items(), key:lambda x:-x[1])]\u001b[0m\n\u001b[0m                                                             ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "biy8kA1dCAVR"
      },
      "source": [
        "# km = KMeans(n_clusters=3)\n",
        "# # inertia_arr = np.array(y).reshape(-1,1)\n",
        "\n",
        "# km.fit(x,y)\n",
        "\n",
        "# check how many unique labels do you have\n",
        "# np.unique(km.labels_)\n",
        "# #array([0, 1, 2], dtype=int32)\n",
        "\n",
        "# km.labels_"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FznpHmrfPuzP"
      },
      "source": [
        "# í´ëŸ¬ìŠ¤í„°ë§ëœ ë¬¸ì„œë“¤ ì¤‘ì—ì„œ íŠ¹ì • ë¬¸ì„œë¥¼ í•˜ë‚˜ ì„ íƒí•œ í›„ ë¹„ìŠ·í•œ ë¬¸ì„œ ì¶”ì¶œ\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "hotel_idx = document_df[document_df['cluster_label']==1].index\n",
        "print(\"í˜¸í…” ì¹´í…Œê³ ë¦¬ë¡œ í´ëŸ¬ìŠ¤í„°ë§ëœ ë¬¸ì„œë“¤ì˜ ì¸ë±ìŠ¤:\\n\",hotel_idx)\n",
        "print()\n",
        "# í˜¸í…” ì¹´í…Œê³ ë¦¬ë¡œ í´ëŸ¬ìŠ¤í„°ë§ ëœ ë¬¸ì„œë“¤ì˜ ì¸ë±ìŠ¤ ì¤‘ í•˜ë‚˜ ì„ íƒí•´ ë¹„êµ ê¸°ì¤€ìœ¼ë¡œ ì‚¼ì„ ë¬¸ì„œ ì„ ì •\n",
        "comparison_doc = document_df.iloc[hotel_idx[0]]['filename']\n",
        "print(\"##ìœ ì‚¬ë„ ë¹„êµ ê¸°ì¤€ ë¬¸ì„œ ì´ë¦„:\",comparison_doc,'##')\n",
        "print()\n",
        "\n",
        "# ìœ„ì—ì„œ ì¶”ì¶œí•œ í˜¸í…” ì¹´í…Œê³ ë¦¬ë¡œ í´ëŸ¬ìŠ¤í„°ë§ëœ ë¬¸ì„œë“¤ì˜ ì¸ë±ìŠ¤ ì¤‘ 0ë²ˆì¸ë±ìŠ¤(ë¹„êµê¸°ì¤€ë¬¸ì„œ)ì œì™¸í•œ\n",
        "# ë‹¤ë¥¸ ë¬¸ì„œë“¤ê³¼ì˜ ìœ ì‚¬ë„ ì¸¡ì •\n",
        "similarity = cosine_similarity(ftr_vect[hotel_idx[0]], ftr_vect[hotel_idx])\n",
        "print(similarity)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AOxqJbsKP3N8"
      },
      "source": [
        "# ë¹„êµê¸°ì¤€ ë¬¸ì„œì™€ ë‹¤ë¥¸ ë¬¸ì„œë“¤ê°„ì˜ ìœ ì‚¬ë„ ì‚´í´ë³´ê¸°\n",
        "import seaborn as sns\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "# array ë‚´ë¦¼ì°¨ìˆœìœ¼ë¡œ ì •ë ¬í•œ í›„ ì¸ë±ìŠ¤ ë°˜í™˜ [:,::-1] ëª¨ë“ í–‰ì— ëŒ€í•´ì„œ ì—´ì„ ë‚´ë¦¼ì°¨ìˆœìœ¼ë¡œ!\n",
        "sorted_idx = similarity.argsort()[:,::-1]\n",
        "# ë¹„êµë¬¸ì„œ ë‹¹ì‚¬ìëŠ” ì œì™¸í•œ ì¸ë±ìŠ¤ ì¶”ì¶œ\n",
        "sorted_idx = sorted_idx[:, 1:]\n",
        "\n",
        "# ìœ ì‚¬ë„ê°€ í° ìˆœìœ¼ë¡œ hotel_idx(label=1ì¸ ì¦‰, í˜¸í…”ê³¼ê´€ë ¨ëœ ë‚´ìš©ì˜ ë¬¸ì„œì´ë¦„ë“¤ì˜ indexë“¤)ì—ì„œ ì¬ ì •ë ¬ \n",
        "# indexë¡œ ë„£ìœ¼ë ¤ë©´ 1ì°¨ì›ìœ¼ë¡œ reshapeí•´ì£¼ê¸°!\n",
        "hotel_sorted_idx = hotel_idx[sorted_idx.reshape(-1,)]\n",
        "# ìœ ì‚¬ë„ í–‰ë ¬ê°’ë“¤ì„ ìœ ì‚¬ë„ê°€ í° ìˆœìœ¼ë¡œ ì¬ì •ë ¬(ë¹„êµ ë¬¸ì„œ ë‹¹ì‚¬ìëŠ” ì œì™¸)\n",
        "hotel_sim_values = np.sort(similarity.reshape(-1,))[::-1]\n",
        "hotel_sim_values = hotel_sim_values[1:]\n",
        "# ì´ë ‡ê²Œ ë˜ë©´ ë¹„êµë¬¸ì„œì™€ ê°€ì¥ ìœ ì‚¬í•œ ìˆœìœ¼ë¡œ 'í•´ë‹¹ë¬¸ì„œì˜index-ìœ ì‚¬ë„ê°’' ìœ¼ë¡œ ë™ì¼í•œ ìœ„ì¹˜ê°€ ë§¤í•‘ëœ ë‘ ê°œì˜ array!\n",
        "# ê·¸ë˜ì„œ ê·¸ëŒ€ë¡œ ë°ì´í„°í”„ë ˆì„ì˜ ê° ì¹¼ëŸ¼ìœ¼ë¡œ ë„£ì–´ì£¼ê¸°\n",
        "print(hotel_sorted_idx)\n",
        "print(hotel_sim_values)\n",
        "print()\n",
        "print(\"ê¸¸ì´ ë¹„êµ\", len(hotel_sorted_idx), len(hotel_sim_values))\n",
        "print()\n",
        "# ë¹ˆ ë°ì´í„°í”„ë ˆì„ ìƒì„±\n",
        "hotel_sim_df = pd.DataFrame()\n",
        "# hotel_sorted_idx ì™€ hotel_sim_values ë§¤í•‘ì‹œí‚¨ arrayì„\n",
        "hotel_sim_df['filename'] = document_df.iloc[hotel_sorted_idx]['filename']\n",
        "hotel_sim_df['similarity'] = hotel_sim_values\n",
        "\n",
        "plt.figure(figsize=(15,10))\n",
        "sns.barplot(data=hotel_sim_df, x='similarity', y='filename')\n",
        "plt.title(comparison_doc)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}