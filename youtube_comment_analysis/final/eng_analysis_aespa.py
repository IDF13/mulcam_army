#!/usr/bin/env python
# coding: utf-8

# # í•„ìš” ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„¤ì¹˜

# In[1]:


get_ipython().system('pip install pandas')
get_ipython().system('pip install pyLDAvis')
get_ipython().system('pip install warnings')


# # import

# In[2]:


import pandas as pd
import re
import nltk
import numpy as np
from nltk.corpus import stopwords 
from nltk.tokenize import word_tokenize
from collections import Counter
import wordcloud
from wordcloud import WordCloud
import matplotlib.pyplot as plt
from sklearn.decomposition import TruncatedSVD
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.decomposition import LatentDirichletAllocation
from sklearn.feature_extraction.text import CountVectorizer
from nltk.tokenize import sent_tokenize
import pyLDAvis.sklearn  # sklearnì˜ ldamodelì— ìµœì í™”ëœ ë¼ì´ë¸ŒëŸ¬ë¦¬
from sklearn.decomposition import PCA
import pyLDAvis.gensim_models
import gensim
from gensim import corpora
from gensim.models import LdaModel
import pyLDAvis
import pyLDAvis.gensim_models as gensimvis
from sklearn.feature_extraction.text import TfidfVectorizer
import nltk
import string
from sklearn.cluster import KMeans
import warnings
from PIL import Image


# In[3]:


warnings.filterwarnings(action='ignore')


# # íŒŒì¼ ë¡œë“œ

# In[4]:


# ìœ íŠœë¸Œ í¬ë¡¤ë§ íŒŒì¼ ë¡œë“œ
path = '/home/lab10/final/pre/'

comment_file = 'prepro_comments_aespa.csv'    #
data = pd.read_csv(path+comment_file, encoding='utf-8', header=None)
data.columns = ['comment','like','lang']
print(len(data))
data.head()


# In[5]:


# data.like.describe(percentiles=[0.75])
# ì¢‹ì•„ìš” ê°¯ìˆ˜ ì¼ì •ê°¯ìˆ˜ ì´ìƒ ë§Œ 
# idx=data[data['like']<=323].index              #ì¢‹ì•„ìš” ê°¯ìˆ˜ ìƒìœ„ 15000 ì •ë„ ì´ìƒ ëŒ“ê¸€ë§Œ ë‚¨ê¹€
# data.drop(idx, inplace=True)

# data_2 = data[data.like >=100]
# len(data_2)

data_ko = pd.DataFrame([kor[:1] for kor in data.values if kor[2] == '(ko)'], columns=['comment'])
data_en = pd.DataFrame([en[:1] for en in data.values if en[2] == '(en)'], columns=['comment'])
data_en.comment.values


# In[6]:


data_ko.comment.values


# In[7]:


for i in range(len(data_en.comment)):
    data_en.comment[i] = str(data_en.comment[i])


# In[8]:


# ìˆ«ìì œê±° / ë°‘ì¤„ ì œì™¸í•œ íŠ¹ìˆ˜ë¬¸ì ì œê±°
p = re.compile("[0-2]+")
z = re.compile("[3-9]+")
q = re.compile("\W+")
r = re.compile('[^a-zA-Z]+')

en = []
for i in data_en.comment.values:
    tokens = re.sub(p," ",i)
    tokens = re.sub(z," ",tokens)
    tokens = re.sub(q," ",tokens)
    tokens = re.sub(r," ", tokens)
    en.append(tokens)
len(en)


# In[9]:


en[:2]


# # ë¶ˆìš©ì–´ ì œê±°

# In[10]:


nltk.download('stopwords')
nltk.download('punkt')
nltk.download('averaged_perceptron_tagger')


# In[11]:


stop_words = set(stopwords.words('english')) 
# stop_words.update(('song','group','songs','youtube','views','time','https','girl','girls','people','yes','lol','video','part','member','members', 'look','way','guys','fans','fan'))
# stop

res=[]
for i in range(len(en)):
    word_tokens = word_tokenize(en[i])

    result = []
    for w in word_tokens: 
        if w not in stop_words: 
            result.append(w) 
    res.append(result)

# print(word_tokens) 
print(res[:10])
print(len(res))


# In[12]:


en_pos = []
for i in range(len(res)):
    tokens_pos = nltk.pos_tag(res[i])
    en_pos.append(tokens_pos)

en_pos[:5]


# In[13]:


# ëª…ì‚¬ëŠ” NNì„ í¬í•¨í•˜ê³  ìˆìŒì„ ì•Œ ìˆ˜ ìˆìŒ
en_NN=[]
for i in range(len(en_pos)):
    NN_words = []
    for word, pos in en_pos[i]:
        if 'NN' in pos:
            NN_words.append(word)
        elif 'NNS' in pos:
            NN_words.append(word)
    en_NN.extend(NN_words)
en_NN[:10]


# In[14]:


#9. ë¹ˆë„ë¶„ì„

c = Counter(en_NN) # input type should be a list of words (or tokens)
k = 20
print(c.most_common(k)) # ë¹ˆë„ìˆ˜ ê¸°ì¤€ ìƒìœ„ kê°œ ë‹¨ì–´ ì¶œë ¥


# In[15]:


#wordclound
noun_text = ''
for word in en_NN:
    noun_text = noun_text +' '+word


# In[16]:


path2='/home/lab10/final/'
filename = comment_file


# In[17]:


# youtube=np.array(Image.open('/home/lab10/final/pngwing.com (4).png'))
# wordcloud = WordCloud(font_path='font/NanumGothic.ttf', background_color='black', colormap='YlOrRd', relative_scaling=.5, mask=youtube).generate(noun_text) # generate() ëŠ” í•˜ë‚˜ì˜ string valueë¥¼ ì…ë ¥ ë°›ìŒ
wordcloud = WordCloud(font_path='font/NanumGothic.ttf', background_color='black', colormap='YlOrRd', relative_scaling=.5).generate(noun_text) # generate() ëŠ” í•˜ë‚˜ì˜ string valueë¥¼ ì…ë ¥ ë°›ìŒ
plt.figure()
plt.imshow(wordcloud, interpolation='bilinear')
plt.axis("off")
plt.show()


# In[18]:


wordcloud.to_file(path2+'wordcloud'+'_en_'+filename+'.png')


# In[19]:


## 3ë‹¨ì–´ ì´í•˜ ì§§ì€ ë‹¨ì–´ ì œê±°
 # remove words less than three letters
# print(res[1])
# for word in res[1]:
#     print(word)
en_sent_less3=[]
for i in range(len(res)):
    tokens = [word for word in res[i] if len(word) >= 3]
    en_sent_less3.append(tokens)
en_sent_less3[:2]


# In[20]:


en_sent =[]
for i in range(len(en_sent_less3)):
    temp=" ".join(en_sent_less3[i])
    en_sent.append(temp)
en_sent[:15]


# In[21]:


data_en['en_sent']=en_sent


# In[22]:


data_en.tail()


# In[23]:


# BoW ëª¨ë¸ë¡œ ë²¡í„°í™”
count = CountVectorizer(ngram_range=(3,6),
                        max_df = 0.05,
                        max_features=10000, stop_words=None)
docs = en_sent
bag = count.fit_transform(docs)


# In[24]:


'''
def perplexity_by_ntopic(data, ntopics):
    output_dict = {
        "Number Of Topics": [],
        "Perplexity Score": []
    }
    for t in ntopics:
        # LDA ì‚¬ìš© (BoW ê¸°ë°˜)
        
        lda = LatentDirichletAllocation(
            n_components=t,
            learning_method='batch',
            random_state=1
        )
        lda.fit(data)

        output_dict['Number Of Topics'].append(t)
        output_dict['Perplexity Score'].append(lda.perplexity(data))

    output_df = pd.DataFrame(output_dict)

    index_min_perplexity = output_df['Perplexity Score'].idxmin()
    output_num_topics = output_df.loc[
        index_min_perplexity, # ì¸ë±ìŠ¤
        "Number Of Topics" # ì»¬ëŸ¼
    ]
    return (output_df, output_num_topics)

df_perplexity, optimal_num_topics = perplexity_by_ntopic(
    bag, ntopics=range(1,100)
)
print(df_perplexity)

df_perplexity[:10]

df_perplexity.sort_values(by=['Perplexity Score'], axis=0)
print(df_perplexity['Perplexity Score'].min())
print(df_perplexity['Perplexity Score'].idxmin())

df_perplexity.plot.line("Number Of Topics",'Perplexity Score')
'''


# In[25]:


"""# ì ì¬ ë””ë¦¬í´ë ˆ í• ë‹¹ì„ ì‚¬ìš©í•œ í† í”½ ëª¨ë¸ë§"""

# LDA ì‚¬ìš© (BoW ê¸°ë°˜)

lda = LatentDirichletAllocation(n_components = 5,
                                random_state = 1,
                                learning_method = 'batch')

X_topics = lda.fit_transform(bag)


# In[26]:


# ê²°ê³¼ ë¶„ì„ì„ ìœ„í•´ ê° í† í”½ ë‹¹ ì¤‘ìš” ë‹¨ì–´ 10ê°œ ì¶œë ¥ (BoW ê¸°ë°˜)
n_top_word = 10
feature_name = count.get_feature_names()
for topic_idx, topic in enumerate(lda.components_):
  print("í† í”½ %d:" % (topic_idx+1))
  print([feature_name[i] for i in topic.argsort()[:-n_top_word - 1: -1]])


# In[27]:


f = open(path2+comment_file+'_en.txt', 'w')
for topic_idx, topic in enumerate(lda.components_):
    a="í† í”½ %d:" % (topic_idx+1)
    f.write('\n'+a)
    b=[]
    for i in topic.argsort()[:-n_top_word - 1: -1]:
        f.write(feature_name[i])
        b.append(feature_name[i])    
    
f.close()


# In[28]:


pyLDAvis.enable_notebook()
vis = pyLDAvis.sklearn.prepare(lda, bag, count)
pyLDAvis.display(vis)


# In[29]:


pyLDAvis.save_html(vis, path2+filename+'lda.html')


# #  í† í¬ë‚˜ì´ì¦ˆí•´ì„œ dictionary ë§Œë“  í›„ ì‘ì—…, ì¦‰ ë‹¨ì–´ 1ê°œì”©

# In[30]:


# gamma, _ = lda.inference(corpus)


# In[31]:


en_sent[:10]


# In[32]:


model = LatentDirichletAllocation(n_components = 5,
                                random_state = 1,
                                learning_method = 'batch')

model.fit(bag) # model.fit_transform(X) is also available


# In[33]:


tokenized_doc = data_en['en_sent'].apply(lambda x: x.split()) # í† í°í™”


# In[34]:


tokenized_doc


# In[35]:


vectorizer = TfidfVectorizer(stop_words='english',
                        ngram_range=(3,6), # ìœ ë‹ˆê·¸ë¨ ë°”ì´ê·¸ë¨ìœ¼ë¡œ ì‚¬ìš©
                        min_df = 3, # 3íšŒ ë¯¸ë§Œìœ¼ë¡œ ë“±ì¥í•˜ëŠ” í† í°ì€ ë¬´ì‹œ
                        max_df =10000# ë§ì´ ë“±ì¥í•œ ë‹¨ì–´ 5%ì˜ í† í°ë„ ë¬´ì‹œ
)

X = vectorizer.fit_transform(en_sent)
X.shape # TF-IDF í–‰ë ¬ì˜ í¬ê¸° í™•ì¸


# In[36]:


svd_model = TruncatedSVD(n_components=5, algorithm='randomized', n_iter=100, random_state=122)
svd_model.fit(X)
len(svd_model.components_)


# In[37]:


np.shape(svd_model.components_)


# In[38]:


# CountVectorizerê°ì²´ë‚´ì˜ ì „ì²´ wordë“¤ì˜ ëª…ì¹­ì„ get_features_names( )ë¥¼ í†µí•´ ì¶”ì¶œ

terms = vectorizer.get_feature_names() # 

def get_topics(components, feature_names, n=10):
    for idx, topic in enumerate(components):
        print("Topic %d:" % (idx+1), [(feature_names[i], topic[i].round(5)) for i in topic.argsort()[:-n - 1:-1]])
get_topics(svd_model.components_,terms)


# In[39]:


dictionary = corpora.Dictionary(tokenized_doc)
corpus = [dictionary.doc2bow(text) for text in tokenized_doc]
print(corpus[1]) # ìˆ˜í–‰ëœ ê²°ê³¼ì—ì„œ ë‘ë²ˆì§¸ ë‰´ìŠ¤ ì¶œë ¥. ì²«ë²ˆì§¸ ë¬¸ì„œì˜ ì¸ë±ìŠ¤ëŠ” 0


# In[40]:


print(dictionary[12])


# In[41]:


len(dictionary)


# In[42]:


NUM_TOPICS = 5 #5ê°œì˜ í† í”½, k=5
ldamodel = gensim.models.ldamodel.LdaModel(corpus, num_topics = NUM_TOPICS, id2word=dictionary, passes=15)
topics = ldamodel.print_topics(num_words=10)
for topic in topics:
    print(topic)


# In[43]:


f = open(path2+comment_file+'en_2.txt', 'w')
f.write(str(topics))
f.close()


# In[44]:


pyLDAvis.enable_notebook()
vis2 = pyLDAvis.gensim_models.prepare(ldamodel, corpus, dictionary)
pyLDAvis.display(vis2)


# In[45]:


pyLDAvis.save_html(vis2, path2+comment_file+'lda_dic.html')


# In[46]:


'''
gamma, _ = ldamodel.inference(corpus)

# ì°¨ì›ì¶•ì†Œ?
topic_vector = ldamodel.expElogbeta
y = PCA(n_components=2).fit_transform(topic_vector)

print('{} -> {}'.format(topic_vector.shape, y.shape))
# (n_topics, n_terms) -> (n_topics, 2)

for i, topic_list in enumerate(ldamodel[corpus]):
    if i==5:
        break
    print(i,'ë²ˆì§¸ ë¬¸ì„œì˜ topic ë¹„ìœ¨ì€',topic_list)

def make_topictable_per_doc(ldamodel, corpus):
    topic_table = pd.DataFrame()

    # ëª‡ ë²ˆì§¸ ë¬¸ì„œì¸ì§€ë¥¼ ì˜ë¯¸í•˜ëŠ” ë¬¸ì„œ ë²ˆí˜¸ì™€ í•´ë‹¹ ë¬¸ì„œì˜ í† í”½ ë¹„ì¤‘ì„ í•œ ì¤„ì”© êº¼ë‚´ì˜¨ë‹¤.
    for i, topic_list in enumerate(ldamodel[corpus]):
        doc = topic_list[0] if ldamodel.per_word_topics else topic_list            
        doc = sorted(doc, key=lambda x: (x[1]), reverse=True)
        # ê° ë¬¸ì„œì— ëŒ€í•´ì„œ ë¹„ì¤‘ì´ ë†’ì€ í† í”½ìˆœìœ¼ë¡œ í† í”½ì„ ì •ë ¬í•œë‹¤.
        # EX) ì •ë ¬ ì „ 0ë²ˆ ë¬¸ì„œ : (2ë²ˆ í† í”½, 48.5%), (8ë²ˆ í† í”½, 25%), (10ë²ˆ í† í”½, 5%), (12ë²ˆ í† í”½, 21.5%), 
        # Ex) ì •ë ¬ í›„ 0ë²ˆ ë¬¸ì„œ : (2ë²ˆ í† í”½, 48.5%), (8ë²ˆ í† í”½, 25%), (12ë²ˆ í† í”½, 21.5%), (10ë²ˆ í† í”½, 5%)
        # 48 > 25 > 21 > 5 ìˆœìœ¼ë¡œ ì •ë ¬ì´ ëœ ê²ƒ.

        # ëª¨ë“  ë¬¸ì„œì— ëŒ€í•´ì„œ ê°ê° ì•„ë˜ë¥¼ ìˆ˜í–‰
        for j, (topic_num, prop_topic) in enumerate(doc): #  ëª‡ ë²ˆ í† í”½ì¸ì§€ì™€ ë¹„ì¤‘ì„ ë‚˜ëˆ ì„œ ì €ì¥í•œë‹¤.
            if j == 0:  # ì •ë ¬ì„ í•œ ìƒíƒœì´ë¯€ë¡œ ê°€ì¥ ì•ì— ìˆëŠ” ê²ƒì´ ê°€ì¥ ë¹„ì¤‘ì´ ë†’ì€ í† í”½
                topic_table = topic_table.append(pd.Series([int(topic_num), round(prop_topic,4), topic_list]), ignore_index=True)
                # ê°€ì¥ ë¹„ì¤‘ì´ ë†’ì€ í† í”½ê³¼, ê°€ì¥ ë¹„ì¤‘ì´ ë†’ì€ í† í”½ì˜ ë¹„ì¤‘ê³¼, ì „ì²´ í† í”½ì˜ ë¹„ì¤‘ì„ ì €ì¥í•œë‹¤.
            else:
                break
    return(topic_table)

topictable = make_topictable_per_doc(ldamodel, corpus)
topictable = topictable.reset_index() # ë¬¸ì„œ ë²ˆí˜¸ì„ ì˜ë¯¸í•˜ëŠ” ì—´(column)ë¡œ ì‚¬ìš©í•˜ê¸° ìœ„í•´ì„œ ì¸ë±ìŠ¤ ì—´ì„ í•˜ë‚˜ ë” ë§Œë“ ë‹¤.
topictable.columns = ['ë¬¸ì„œ ë²ˆí˜¸', 'ê°€ì¥ ë¹„ì¤‘ì´ ë†’ì€ í† í”½', 'ê°€ì¥ ë†’ì€ í† í”½ì˜ ë¹„ì¤‘', 'ê° í† í”½ì˜ ë¹„ì¤‘']
topictable[:10]

topic_word = model.components_ # model.components_also works
n_top_words = 5   # TOPICìœ¼ë¡œ ì„ ì •ë  ë‹¨ì–´ì˜ ìˆ˜

topic_word
'''


# In[47]:


'''
for i, topic_dist in enumerate(topic_word):
    topic_words = np.array(tokenized_doc)[np.argsort(topic_dist)][:-(n_top_words+1):-1]
    print("Topic {}:{}".format(i, ' '.join(topic_words)))

dictionary = gensim.corpora.Dictionary(bag)
print('dictionary size : %d' % len(dictionary)) # dictionary size : ????

min_count = 10
word_counter = Counter((word for words in documents for word in words))
removal_word_idxs = {
    dictionary.token2id[word] for word, count in word_counter.items()
    if count < min_count
}

dictionary.filter_tokens(removal_word_idxs)
dictionary.compactify()
print('dictionary size : %d' % len(dictionary)) # dictionary size : 

lda_model = LdaModel(bag, id2word=dictionary, num_topics=6)
with open(lda_model_path, 'wb') as f:
    pickle.dump(lda_model, f)
    '''


# In[48]:


'''
def get_topic_term_prob(lda_model):
    topic_term_freqs = lda_model.state.get_lambda()
    topic_term_prob = topic_term_freqs / topic_term_freqs.sum(axis=1)[:, None]
    return topic_term_prob

print(ldamodel.alpha.shape) # (n_topics,)
print(ldamodel.alpha.sum()) # 1.0

topic_term_prob = get_topic_term_prob(ldamodel)
print(topic_term_prob.shape)     # (n_topics, n_terms)
print(topic_term_prob[0].sum())  # 1.0
'''


# In[ ]:





# In[ ]:





# In[ ]:





# In[49]:


# # TF_IDF ë²¡í„°í™”

# tfidf = TfidfVectorizer(ngram_range=(3,6), # ìœ ë‹ˆê·¸ë¨ ë°”ì´ê·¸ë¨ìœ¼ë¡œ ì‚¬ìš©
#                         min_df = 3, # 3íšŒ ë¯¸ë§Œìœ¼ë¡œ ë“±ì¥í•˜ëŠ” í† í°ì€ ë¬´ì‹œ
#                         max_df =0.95 # ë§ì´ ë“±ì¥í•œ ë‹¨ì–´ 5%ì˜ í† í°ë„ ë¬´ì‹œ
#                         )
# docs_tf = tfidf.fit_transform(docs)


# # In[68]:
# # LDA ì‚¬ìš© (tf-idf ê¸°ë°˜)
# lda_tfidf = LatentDirichletAllocation(n_components = 6,
#                                       random_state = 1,
#                                       learning_method = 'batch')

# X_topics = lda_tfidf.fit_transform(docs_tf)


# # In[37]:


# # ê²°ê³¼ ë¶„ì„ì„ ìœ„í•´ ê° í† í”½ ë‹¹ ì¤‘ìš” ë‹¨ì–´ 10ê°œ ì¶œë ¥ (tf-idf ê¸°ë°˜)
# n_top_word = 5
# feature_name = count.get_feature_names()
# for topic_idx, topic in enumerate(lda_tfidf.components_):
#   print("í† í”½ %d:" % (topic_idx+1))
#   print([feature_name[i] for i in topic.argsort()[:-n_top_word - 1: -1]])


# # rake

# In[50]:


'''get_ipython().system('pip install rake-nltk')


from rake_nltk import Rake

# raw= pd.read_csv('/content/drive/MyDrive/Colab Notebooks/[á„€á…©á†¼á„‹á…²] Mulcam_Army á„€á…©á†¼á„‹á…²á„‘á…©á†¯á„ƒá…¥!/á„á…³á„…á…©á†¯á„…á…µá†¼ á„’á…¡á†« á„Œá…¡á„…á…­/youtube/á„‹á…§á†¼á„‰á…¡á†¼á„‡á…§á†¯ á„ƒá…¢á†ºá„€á…³á†¯/comments_youtube_aespa.csv',header=None)
# raw.columns=['comments','likes']
# raw

en_sent[:10]

text=". ".join(en_sent)

r=Rake()
r.extract_keywords_from_text(text)
r.get_ranked_phrases_with_scores()[:5]

r1=Rake()
r1.extract_keywords_from_sentences(en_sent[:10])
r1.get_ranked_phrases_with_scores()
'''


# # yake

# In[51]:


'''
# In[42]:


get_ipython().system('pip install git+https://github.com/LIAAD/yake')


# In[43]:


import yake


# In[44]:


a=". ".join(en_sent)


# In[ ]:


a


# In[74]:


language = "en"
max_ngram_size = 2
deduplication_thresold = 0.9
deduplication_algo = 'seqm'
windowSize = 3
numOfKeywords = 20

custom_kw_extractor = yake.KeywordExtractor(lan=language, n=max_ngram_size, dedupLim=deduplication_thresold, dedupFunc=deduplication_algo, windowsSize=windowSize, top=numOfKeywords, features=None)
keywords = custom_kw_extractor.extract_keywords(a)

for kw in keywords:
    print(kw)
'''


# # pke

# In[52]:


'''get_ipython().system('pip install git+https://github.com/boudinfl/pke.git')
get_ipython().system('pip install spacy')
get_ipython().system('python3 -m spacy download en')
'''


# In[53]:


'''
import pke
import spacy
# nlp = spacy.load("en_core_web_sm")

for i in range(len(en_sent)):

    # initialize keyphrase extraction model, here TopicRank
    extractor = pke.unsupervised.TopicRank()

    # load the content of the document, here document is expected to be in raw
    # # format (i.e. a simple text file) and preprocessing is carried out using spacy
    extractor.load_document(input=en_sent[i], language='en_core_web_sm')
    extractor.candidate_selection(pos={'NOUN'})
    extractor.candidate_weighting(threshold=0.74,
                                  method='average')
    keyphrase_score=[]
    for (keyphrase, score) in extractor.get_n_best(n=2, stemming=False):
        print(keyphrase, score)
        keyphrase_score.extend((keyphrase, score))
        

keyphrase_score

with open('/home/lab10/final/key_score.txt', 'w') as f:
    for (keyphrase, score) in keyphrase_score:
        f.write((keyphrase, score))
        '''


# # k-means

# ## ìƒìœ„ 10% ëŒ“ê¸€ 

# In[54]:


data.like.describe(percentiles=[0.9])
                # ì¢‹ì•„ìš” ê°¯ìˆ˜ ì¼ì •ê°¯ìˆ˜ ì´ìƒ ë§Œ 


# In[55]:


# idx=data[data['like']<=185].index              #ì¢‹ì•„ìš” ê°¯ìˆ˜ ìƒìœ„ 15000 ì •ë„ ì´ìƒ ëŒ“ê¸€ë§Œ ë‚¨ê¹€
# data.drop(idx, inplace=True)

data_2 = data[data.like >=185]
len(data_2)


# In[56]:


data_ko = pd.DataFrame([kor[:1] for kor in data_2.values if kor[2] == '(ko)'], columns=['comment'])
data_en = pd.DataFrame([en[:1] for en in data_2.values if en[2] == '(en)'], columns=['comment'])
data_en.comment.values


# In[57]:


for i in range(len(data_en.comment)):
    data_en.comment[i] = str(data_en.comment[i])


# In[58]:


# ìˆ«ìì œê±° / ë°‘ì¤„ ì œì™¸í•œ íŠ¹ìˆ˜ë¬¸ì ì œê±°
p = re.compile("[0-2]+")
z = re.compile("[3-9]+")
q = re.compile("\W+")
r = re.compile('[^a-zA-Z]+')

en = []
for i in data_en.comment.values:
    tokens = re.sub(p," ",i)
    tokens = re.sub(z," ",tokens)
    tokens = re.sub(q," ",tokens)
    tokens = re.sub(r," ", tokens)
    en.append(tokens)
len(en)


# In[59]:


stop_words = set(stopwords.words('english')) 
# stop_words.update(('song','group','songs','youtube','views','time','https','girl','girls','people','yes','lol','video','part','member','members', 'look','way','guys','fans','fan'))
# stop

res=[]
for i in range(len(en)):
    word_tokens = word_tokenize(en[i])

    result = []
    for w in word_tokens: 
        if w not in stop_words: 
            result.append(w) 
    res.append(result)

# print(word_tokens) 
print(res[:10])
print(len(res))


# In[60]:


## 3ë‹¨ì–´ ì´í•˜ ì§§ì€ ë‹¨ì–´ ì œê±°
 # remove words less than three letters
# print(res[1])
# for word in res[1]:
#     print(word)
en_sent_less3=[]
for i in range(len(res)):
    tokens = [word for word in res[i] if len(word) >= 3]
    en_sent_less3.append(tokens)
en_sent_less3[:2]


# In[61]:


en_sent =[]
for i in range(len(en_sent_less3)):
    temp=" ".join(en_sent_less3[i])
    en_sent.append(temp)
en_sent[:15]


# In[62]:


data_en['en_sent']=en_sent


# In[63]:


data_en.tail()


# ## ìƒìœ„ 10% k-means í´ëŸ¬ìŠ¤í„°ë§

# In[64]:


# Tf-idf ë²¡í„°í™”ì‹œí‚¤ë©´ì„œ cusotmizedí•´ì¤€ í† í°í™”+ì–´ê·¼ì¶”ì¶œ ë°©ì‹ tokenizerì¸ìì— ë„£ì–´ì£¼ê¸°
# ë²¡í„°í™”ì‹œí‚¬ Tf-idf ë„êµ¬ ì˜µì…˜ ì¶”ê°€í•´ì„œ êµ¬ì¶•
# 1,3gramì ìš©, ë¹ˆë„ìˆ˜ 0.05ì´í•˜, 0.95ì´ìƒì˜ ë¹ˆë„ìˆ˜ ë‹¨ì–´ë“¤ ì œê±°
tfidf_vect = TfidfVectorizer(ngram_range=(1,3),
                            min_df=0.05, max_df=0.95)
# fit_transformìœ¼ë¡œ ìœ„ì—ì„œ êµ¬ì¶•í•œ ë„êµ¬ë¡œ í…ìŠ¤íŠ¸ ë²¡í„°í™”
ftr_vect = tfidf_vect.fit_transform(en_sent)

# K-meansë¡œ 6ê°œ êµ°ì§‘ìœ¼ë¡œ ë¬¸ì„œ êµ°ì§‘í™”ì‹œí‚¤ê¸°

kmeans = KMeans(n_clusters=5, max_iter=10000, random_state=42)
# ë¹„ì§€ë„ í•™ìŠµì´ë‹ˆ featureë¡œë§Œ í•™ìŠµì‹œí‚¤ê³  ì˜ˆì¸¡
cluster_label = kmeans.fit_predict(ftr_vect)

# êµ°ì§‘í™”í•œ ë ˆì´ë¸”ê°’ë“¤ì„ document_df ì— ì¶”ê°€í•˜ê¸°
data_en['label'] = cluster_label
print(data_en.sort_values(by=['label']))


# In[65]:


# ë¬¸ì„œì˜ feature(ë‹¨ì–´ë³„) cluster_centers_í™•ì¸í•´ë³´ì
cluster_centers = kmeans.cluster_centers_
print(cluster_centers.shape)
print(cluster_centers)
# shapeì˜ í–‰ì€ í´ëŸ¬ìŠ¤í„° ë ˆì´ë¸”, ì—´ì€ ë²¡í„°í™” ì‹œí‚¨ feature(ë‹¨ì–´ë“¤)


# In[66]:


def get_cluster_details(cluster_model, cluster_data, feature_names,
                       cluster_num, top_n_features=5):
    cluster_details = {}
    # ê° í´ëŸ¬ìŠ¤í„° ë ˆì´ë¸”ë³„ featureë“¤ì˜ centerê°’ë“¤ ë‚´ë¦¼ì°¨ìˆœìœ¼ë¡œ ì •ë ¬ í›„ì˜ ì¸ë±ìŠ¤ë¥¼ ë°˜í™˜
    center_feature_idx = cluster_model.cluster_centers_.argsort()[:,::-1]
    
    # ê°œë³„ í´ëŸ¬ìŠ¤í„° ë ˆì´ë¸”ë³„ë¡œ 
    for cluster_num in range(cluster_num):
        # ê°œë³„ í´ëŸ¬ìŠ¤í„°ë³„ ì •ë³´ë¥¼ ë‹´ì„ empty dictí• ë‹¹
        cluster_details[cluster_num] = {}
        cluster_details[cluster_num]['cluster'] = cluster_num
        
        # ê° featureë³„ centerê°’ë“¤ ì •ë ¬í•œ ì¸ë±ìŠ¤ ì¤‘ ìƒìœ„ 5ê°œë§Œ ì¶”ì¶œ
        top_ftr_idx = center_feature_idx[cluster_num, :top_n_features]
        top_ftr = [feature_names[idx] for idx in top_ftr_idx]
        # top_ftr_idxë¥¼ í™œìš©í•´ì„œ ìƒìœ„ 5ê°œ featureë“¤ì˜ centerê°’ë“¤ ë°˜í™˜
        # ë°˜í™˜í•˜ê²Œ ë˜ë©´ arrayì´ê¸° ë–„ë¬¸ì— ë¦¬ìŠ¤íŠ¸ë¡œë°”ê¾¸ê¸°
        top_ftr_val = cluster_model.cluster_centers_[cluster_num, top_ftr_idx].tolist()
        
        # cluster_details ë”•ì…”ë„ˆë¦¬ì—ë‹¤ê°€ ê°œë³„ êµ°ì§‘ ì •ë³´ ë„£ì–´ì£¼ê¸°
        cluster_details[cluster_num]['top_features'] = top_ftr
        cluster_details[cluster_num]['top_featrues_value'] = top_ftr_val
        # í•´ë‹¹ cluster_numìœ¼ë¡œ ë¶„ë¥˜ëœ íŒŒì¼ëª…(ë¬¸ì„œë“¤) ë„£ì–´ì£¼ê¸°
        comment = cluster_data[cluster_data['label']==cluster_num]['comment']
        # filenamesê°€ dfìœ¼ë¡œ ë°˜í™˜ë˜ê¸° ë–„ë¬¸ì— ê°’ë“¤ë§Œ ì¶œë ¥í•´ì„œ array->listë¡œ ë³€í™˜
        comment = comment.values.tolist()
        cluster_details[cluster_num]['comment'] = comment
    
    return cluster_details

def print_cluster_details(cluster_details):
    for cluster_num, cluster_detail in cluster_details.items():
        print(f"#####Cluster Num: {cluster_num}")
        print()
        print("ìƒìœ„ 5ê°œ featureë‹¨ì–´ë“¤:\n", cluster_detail['top_features'])
        print()
        print(f"Cluster {cluster_num}ìœ¼ë¡œ ë¶„ë¥˜ëœ ë¬¸ì„œë“¤:\n{cluster_detail['comment'][:10]}")
        print('-'*20)

feature_names = tfidf_vect.get_feature_names()
cluster_details = get_cluster_details(cluster_model=kmeans,
                                     cluster_data=data_en,
                                     feature_names=feature_names,
                                     cluster_num=5,
                                     top_n_features=5)
print_cluster_details(cluster_details)


# # ëª…ì‚¬ ì¶”ì¶œ 
# 
# ## ëŒ“ê¸€ êµ°ì§‘í™” / ìì£¼ ë‚˜ì˜¨ / ëŒ“ê¸€ êµ°ì§‘í•œ ê²ƒì„ ê°€ì§€ê³  êµ°ì§‘ë³„ 
# 
# ### í•´ë‹¹ ì•„í‹°ìŠ¤íŠ¸ ì´ë¦„ ì œê±°
# ### word to bag 
# ### êµ°ì§‘ë³„ ì›Œë“œí´ë¼ìš°ë“œ
# ### ê³µê°ì§€ìˆ˜ ê¸°ë°˜ ìƒìœ„ 10% ê°€ì§€ê³  ë˜‘ê°™ì€ ìˆ˜í–‰
# 
# 

# In[67]:


data_en.tail()


# In[68]:


en_pos = []
for i in range(len(res)):
    tokens_pos = nltk.pos_tag(res[i])
    en_pos.append(tokens_pos)


# In[69]:


# ëª…ì‚¬ëŠ” NNì„ í¬í•¨í•˜ê³  ìˆìŒì„ ì•Œ ìˆ˜ ìˆìŒ
en_NN=[]
for i in range(len(en_pos)):
    NN_words = []
    for word, pos in en_pos[i]:
        if 'NN' in pos:
            NN_words.append(word)
        elif 'NNS' in pos:
            NN_words.append(word)
    en_NN.append(NN_words)
en_NN[:10]


# In[70]:


data_en['en_sent']=en_NN


# In[71]:


for i in range(len(data_en.en_sent)):
    data_en.en_sent[i]=' '.join(data_en.en_sent[i])


# In[72]:


data_en


# In[73]:


data_en['en_sent'].str.contains('aespa').value_counts()


# In[74]:


df=data_en.copy()


# In[75]:


df.drop(['comment'],axis=1,inplace=True)


# In[76]:


for i in range(len(df.en_sent)):
    df.en_sent[i] = str(df.en_sent[i])


# In[77]:


# ìˆ«ìì œê±° / ë°‘ì¤„ ì œì™¸í•œ íŠ¹ìˆ˜ë¬¸ì ì œê±°
p = re.compile("aespa")

en1 = []
for i in df.en_sent.values:
    tokens = re.sub(p," ",i)
    en1.append(tokens)
len(en1)


# In[78]:


df['en_sent']=en1


# In[79]:


df['en_sent'].str.contains('aespa').value_counts()


# ### BoW 

# In[80]:


# BoW ëª¨ë¸ë¡œ ë²¡í„°í™”
count = CountVectorizer(ngram_range=(1,3),
                        max_df = 0.05,
                        max_features=5, stop_words='english')
docs2 = df.en_sent
bag2 = count.fit_transform(docs2)


# In[81]:


kmeans = KMeans(n_clusters=5, max_iter=10000, random_state=42)
# ë¹„ì§€ë„ í•™ìŠµì´ë‹ˆ featureë¡œë§Œ í•™ìŠµì‹œí‚¤ê³  ì˜ˆì¸¡
cluster_label = kmeans.fit_predict(bag2)

# êµ°ì§‘í™”í•œ ë ˆì´ë¸”ê°’ë“¤ì„ document_df ì— ì¶”ê°€í•˜ê¸°
df['label'] = cluster_label
print(df.sort_values(by=['label']))


# In[82]:


# ë¬¸ì„œì˜ feature(ë‹¨ì–´ë³„) cluster_centers_í™•ì¸í•´ë³´ì
cluster_centers = kmeans.cluster_centers_
print(cluster_centers.shape)
print(cluster_centers)
# shapeì˜ í–‰ì€ í´ëŸ¬ìŠ¤í„° ë ˆì´ë¸”, ì—´ì€ ë²¡í„°í™” ì‹œí‚¨ feature(ë‹¨ì–´ë“¤)


# In[83]:


def get_cluster_details(cluster_model, cluster_data, feature_names,
                       cluster_num, top_n_features):
    cluster_details = {}
    # ê° í´ëŸ¬ìŠ¤í„° ë ˆì´ë¸”ë³„ featureë“¤ì˜ centerê°’ë“¤ ë‚´ë¦¼ì°¨ìˆœìœ¼ë¡œ ì •ë ¬ í›„ì˜ ì¸ë±ìŠ¤ë¥¼ ë°˜í™˜
    center_feature_idx = cluster_model.cluster_centers_.argsort()[:,::-1]
    
    # ê°œë³„ í´ëŸ¬ìŠ¤í„° ë ˆì´ë¸”ë³„ë¡œ 
    for cluster_num in range(cluster_num):
        # ê°œë³„ í´ëŸ¬ìŠ¤í„°ë³„ ì •ë³´ë¥¼ ë‹´ì„ empty dictí• ë‹¹
        cluster_details[cluster_num] = {}
        cluster_details[cluster_num]['cluster'] = cluster_num
        
        # ê° featureë³„ centerê°’ë“¤ ì •ë ¬í•œ ì¸ë±ìŠ¤ ì¤‘ ìƒìœ„ 5ê°œë§Œ ì¶”ì¶œ
        top_ftr_idx = center_feature_idx[cluster_num, :top_n_features]
        top_ftr = [feature_names[idx] for idx in top_ftr_idx]
        # top_ftr_idxë¥¼ í™œìš©í•´ì„œ ìƒìœ„ 1ê°œ featureë“¤ì˜ centerê°’ë“¤ ë°˜í™˜
        # ë°˜í™˜í•˜ê²Œ ë˜ë©´ arrayì´ê¸° ë–„ë¬¸ì— ë¦¬ìŠ¤íŠ¸ë¡œë°”ê¾¸ê¸°
        top_ftr_val = cluster_model.cluster_centers_[cluster_num, top_ftr_idx].tolist()
        
        # cluster_details ë”•ì…”ë„ˆë¦¬ì—ë‹¤ê°€ ê°œë³„ êµ°ì§‘ ì •ë³´ ë„£ì–´ì£¼ê¸°
        cluster_details[cluster_num]['top_features'] = top_ftr
        cluster_details[cluster_num]['top_featrues_value'] = top_ftr_val
        # í•´ë‹¹ cluster_numìœ¼ë¡œ ë¶„ë¥˜ëœ íŒŒì¼ëª…(ë¬¸ì„œë“¤) ë„£ì–´ì£¼ê¸°
        comment = cluster_data[cluster_data['label']==cluster_num]['en_sent']
        # filenamesê°€ dfìœ¼ë¡œ ë°˜í™˜ë˜ê¸° ë–„ë¬¸ì— ê°’ë“¤ë§Œ ì¶œë ¥í•´ì„œ array->listë¡œ ë³€í™˜
        comment = comment.values.tolist()
        cluster_details[cluster_num]['en_sent'] = comment
    
    return cluster_details

def print_cluster_details(cluster_details):
    for cluster_num, cluster_detail in cluster_details.items():
        print(f"#####Cluster Num: {cluster_num}")
        print()
        print(f"ìƒìœ„ 1ê°œ featureë‹¨ì–´ë“¤:\n", cluster_detail['top_features'])
        print()
        print(f"Cluster {cluster_num}ìœ¼ë¡œ ë¶„ë¥˜ëœ ë¬¸ì„œë“¤:\n{cluster_detail['en_sent'][:10]}")
        print('-'*20)

feature_names = count.get_feature_names()


# In[84]:


cluster_details = get_cluster_details(cluster_model=kmeans,
                                     cluster_data=df,
                                     feature_names=feature_names,
                                     cluster_num=5,
                                     top_n_features=5)


# In[85]:


cluster_details[0]


# In[86]:


print_cluster_details(cluster_details)


# In[ ]:





# ## ê·¸ë£¹ë³„ ì›Œë“œí´ë¼ìš°ë“œ

# In[87]:


a=" ".join(cluster_details[0]['en_sent'])


# In[88]:


wordcloud = WordCloud(font_path='font/NanumGothic.ttf', background_color='black', colormap='YlOrRd', relative_scaling=.5).generate(a) # generate() ëŠ” í•˜ë‚˜ì˜ string valueë¥¼ ì…ë ¥ ë°›ìŒ
plt.figure()
plt.imshow(wordcloud, interpolation='bilinear')
plt.axis("off")
plt.show()


# In[89]:


b=" ".join(cluster_details[1]['en_sent'])


# In[90]:


wordcloud = WordCloud(font_path='font/NanumGothic.ttf', background_color='black', colormap='YlOrRd', relative_scaling=.5).generate(b) # generate() ëŠ” í•˜ë‚˜ì˜ string valueë¥¼ ì…ë ¥ ë°›ìŒ
plt.figure()
plt.imshow(wordcloud, interpolation='bilinear')
plt.axis("off")
plt.show()


# In[91]:


c=" ".join(cluster_details[2]['en_sent'])


# In[92]:


wordcloud = WordCloud(font_path='font/NanumGothic.ttf', background_color='black', colormap='YlOrRd', relative_scaling=.5).generate(c) # generate() ëŠ” í•˜ë‚˜ì˜ string valueë¥¼ ì…ë ¥ ë°›ìŒ
plt.figure()
plt.imshow(wordcloud, interpolation='bilinear')
plt.axis("off")
plt.show()


# In[93]:


d=" ".join(cluster_details[3]['en_sent'])


# In[94]:


wordcloud = WordCloud(font_path='font/NanumGothic.ttf', background_color='black', colormap='YlOrRd', relative_scaling=.5).generate(d) # generate() ëŠ” í•˜ë‚˜ì˜ string valueë¥¼ ì…ë ¥ ë°›ìŒ
plt.figure()
plt.imshow(wordcloud, interpolation='bilinear')
plt.axis("off")
plt.show()


# In[95]:


e=" ".join(cluster_details[4]['en_sent'])


# In[96]:


wordcloud = WordCloud(font_path='font/NanumGothic.ttf', background_color='black', colormap='YlOrRd', relative_scaling=.5).generate(e) # generate() ëŠ” í•˜ë‚˜ì˜ string valueë¥¼ ì…ë ¥ ë°›ìŒ
plt.figure()
plt.imshow(wordcloud, interpolation='bilinear')
plt.axis("off")
plt.show()


# In[ ]:





# In[97]:


get_ipython().system('pip install textblob')


# In[98]:


data


# In[99]:


data_ko = pd.DataFrame([kor[:2] for kor in data.values if kor[2] == '(ko)'], columns=['comment','like'])
data_en = pd.DataFrame([en[:2] for en in data.values if en[2] == '(en)'], columns=['comment','like'])
data_en.comment.values


# In[100]:


data_en.dropna()


# In[101]:


data_en.info()


# In[102]:


data_en.fillna("None",inplace=True)


# In[103]:


data_en.info()


# In[104]:


import pandas as pd
from textblob import TextBlob

# ì—¬ê¸°ë§Œ ë°”ê¿”ì£¼ì‹œë©´ ë©ë‹ˆë‹¤!
texts = data_en.iloc[:, 0]
likes = data_en.iloc[:, 1]


# In[138]:


clean_tweets = []
scores = []
likes_list = []
emotions = []
lang=data['lang']

likes = likes.fillna(0)
value_list = likes.values.tolist()
value_list = [str(value) for value in value_list]


def sentiment_analyzer():
    for tweet in texts:
        clean_tweets.append(tweet)
        blob = TextBlob(tweet)
        score = round(blob.sentiment.polarity, 2)
        scores.append(score)

    # for like in likes:
    #     likes_list.append(like)

    for value in value_list:
        result = 0
        num_map = {'ì²œ': 1000, 'ë§Œ': 10000}
        if value.isdigit():
            result = int(value)
        else:
            if len(value) > 1:
                result = float(value[:-1])*num_map.get(value[-1].upper(), 1)
        likes_list.append(int(result))

    for score, like in zip(scores, likes_list):
        emotions.append(round(score*like, 2))

    table = pd.DataFrame(
        [clean_tweets, likes_list, scores, emotions, lang]).T
    table.columns = ['original texts', 'ğŸ‘ ì¢‹ì•„ìš”',
                     'â­ï¸ ê¸ì •ì ìˆ˜(textblob)', 'ğŸ˜ ê³µê°ì§€ìˆ˜(ì¢‹ì•„ìš” ìˆ˜ x ê¸ì •ì ìˆ˜)','lang']

    print(table)

    table.to_csv(f'/home/lab10/final/'+filename+'_table.csv')
    return table


# In[139]:


sentiment_analyzer()


# In[140]:


df2=pd.read_csv(f'/home/lab10/final/'+filename+'_table.csv')


# In[141]:


df2.sort_values(by=['ğŸ˜ ê³µê°ì§€ìˆ˜(ì¢‹ì•„ìš” ìˆ˜ x ê¸ì •ì ìˆ˜)'], axis=0,ascending=False)


# In[142]:


df2['ğŸ˜ ê³µê°ì§€ìˆ˜(ì¢‹ì•„ìš” ìˆ˜ x ê¸ì •ì ìˆ˜)'].describe(percentiles=[0.9])
                # ì¢‹ì•„ìš” ê°¯ìˆ˜ ì¼ì •ê°¯ìˆ˜ ì´ìƒ ë§Œ 


# In[143]:


# idx=data[df2['ğŸ˜ ê³µê°ì§€ìˆ˜(ì¢‹ì•„ìš” ìˆ˜ x ê¸ì •ì ìˆ˜)']<10].index              #ì¢‹ì•„ìš” ê°¯ìˆ˜ ìƒìœ„ 15000 ì •ë„ ì´ìƒ ëŒ“ê¸€ë§Œ ë‚¨ê¹€
# df2.drop(idx, inplace=True)

df3 = df2[df2['ğŸ˜ ê³µê°ì§€ìˆ˜(ì¢‹ì•„ìš” ìˆ˜ x ê¸ì •ì ìˆ˜)'] >= 5]
len(df3)


# In[144]:


df3


# In[145]:


df3.drop(['Unnamed: 0'],axis=1,inplace=True)


# In[146]:


df3


# In[147]:


data_ko = pd.DataFrame([kor[:1] for kor in df3.values if kor[4] == '(ko)'], columns=['original texts'])
data_en2 = pd.DataFrame([en[:1] for en in df3.values if en[4] == '(en)'], columns=['original texts'])
data_en2['original texts'].values


# In[148]:


for i in range(len(data_en2['original texts'])):
    data_en2['original texts'][i] = str(data_en2['original texts'][i])


# In[149]:


# ìˆ«ìì œê±° / ë°‘ì¤„ ì œì™¸í•œ íŠ¹ìˆ˜ë¬¸ì ì œê±°
p = re.compile("[0-2]+")
z = re.compile("[3-9]+")
q = re.compile("\W+")
r = re.compile('[^a-zA-Z]+')

en = []
for i in data_en2['original texts'].values:
    tokens = re.sub(p," ",i)
    tokens = re.sub(z," ",tokens)
    tokens = re.sub(q," ",tokens)
    tokens = re.sub(r," ", tokens)
    en.append(tokens)
len(en)


# In[150]:


stop_words = set(stopwords.words('english')) 
stop_words.update(('aespa','aespa'))
# stop

res=[]
for i in range(len(en)):
    word_tokens = word_tokenize(en[i])

    result = []
    for w in word_tokens: 
        if w not in stop_words: 
            result.append(w) 
    res.append(result)

# print(word_tokens) 
print(res[:10])
print(len(res))


# In[151]:


en_sent_less3=[]
for i in range(len(res)):
    tokens = [word for word in res[i] if len(word) >= 3]
    en_sent_less3.append(tokens)
en_sent_less3[:2]


# In[152]:


en_sent =[]
for i in range(len(en_sent_less3)):
    temp=" ".join(en_sent_less3[i])
    en_sent.append(temp)
en_sent[:15]


# In[153]:


data_en2['en_sent']=en_sent


# In[154]:


len(data_en2)


# In[155]:


data_en2.tail()


# In[156]:


# BoW ëª¨ë¸ë¡œ ë²¡í„°í™”
count = CountVectorizer(ngram_range=(1,3),
                        max_df = 0.05,
                        max_features=5, stop_words='english')
docs3 = data_en2.en_sent
bag3 = count.fit_transform(docs3)


# In[157]:


kmeans = KMeans(n_clusters=5, max_iter=10000, random_state=42)
# ë¹„ì§€ë„ í•™ìŠµì´ë‹ˆ featureë¡œë§Œ í•™ìŠµì‹œí‚¤ê³  ì˜ˆì¸¡
cluster_label = kmeans.fit_predict(bag3)

# êµ°ì§‘í™”í•œ ë ˆì´ë¸”ê°’ë“¤ì„ document_df ì— ì¶”ê°€í•˜ê¸°
data_en2['label'] = cluster_label
print(data_en2.sort_values(by=['label']))


# In[158]:


# ë¬¸ì„œì˜ feature(ë‹¨ì–´ë³„) cluster_centers_í™•ì¸í•´ë³´ì
cluster_centers = kmeans.cluster_centers_
print(cluster_centers.shape)
print(cluster_centers)
# shapeì˜ í–‰ì€ í´ëŸ¬ìŠ¤í„° ë ˆì´ë¸”, ì—´ì€ ë²¡í„°í™” ì‹œí‚¨ feature(ë‹¨ì–´ë“¤)


# In[168]:


def get_cluster_details(cluster_model, cluster_data, feature_names,
                       cluster_num, top_n_features):
    cluster_details = {}
    # ê° í´ëŸ¬ìŠ¤í„° ë ˆì´ë¸”ë³„ featureë“¤ì˜ centerê°’ë“¤ ë‚´ë¦¼ì°¨ìˆœìœ¼ë¡œ ì •ë ¬ í›„ì˜ ì¸ë±ìŠ¤ë¥¼ ë°˜í™˜
    center_feature_idx = cluster_model.cluster_centers_.argsort()[:,::-1]
    
    # ê°œë³„ í´ëŸ¬ìŠ¤í„° ë ˆì´ë¸”ë³„ë¡œ 
    for cluster_num in range(cluster_num):
        # ê°œë³„ í´ëŸ¬ìŠ¤í„°ë³„ ì •ë³´ë¥¼ ë‹´ì„ empty dictí• ë‹¹
        cluster_details[cluster_num] = {}
        cluster_details[cluster_num]['cluster'] = cluster_num
        
        # ê° featureë³„ centerê°’ë“¤ ì •ë ¬í•œ ì¸ë±ìŠ¤ ì¤‘ ìƒìœ„ 5ê°œë§Œ ì¶”ì¶œ
        top_ftr_idx = center_feature_idx[cluster_num, :top_n_features]
        top_ftr = [feature_names[idx] for idx in top_ftr_idx]
        # top_ftr_idxë¥¼ í™œìš©í•´ì„œ ìƒìœ„ 1ê°œ featureë“¤ì˜ centerê°’ë“¤ ë°˜í™˜
        # ë°˜í™˜í•˜ê²Œ ë˜ë©´ arrayì´ê¸° ë–„ë¬¸ì— ë¦¬ìŠ¤íŠ¸ë¡œë°”ê¾¸ê¸°
        top_ftr_val = cluster_model.cluster_centers_[cluster_num, top_ftr_idx].tolist()
        
        # cluster_details ë”•ì…”ë„ˆë¦¬ì—ë‹¤ê°€ ê°œë³„ êµ°ì§‘ ì •ë³´ ë„£ì–´ì£¼ê¸°
        cluster_details[cluster_num]['top_features'] = top_ftr
        cluster_details[cluster_num]['top_featrues_value'] = top_ftr_val
        # í•´ë‹¹ cluster_numìœ¼ë¡œ ë¶„ë¥˜ëœ íŒŒì¼ëª…(ë¬¸ì„œë“¤) ë„£ì–´ì£¼ê¸°
        comment = cluster_data[cluster_data['label']==cluster_num]['en_sent']
        # filenamesê°€ dfìœ¼ë¡œ ë°˜í™˜ë˜ê¸° ë–„ë¬¸ì— ê°’ë“¤ë§Œ ì¶œë ¥í•´ì„œ array->listë¡œ ë³€í™˜
        comment = comment.values.tolist()
        cluster_details[cluster_num]['en_sent'] = comment
    
    return cluster_details

def print_cluster_details(cluster_details):
    for cluster_num, cluster_detail in cluster_details.items():
        print(f"#####Cluster Num: {cluster_num}")
        print()
        print(f"ìƒìœ„ 1ê°œ featureë‹¨ì–´ë“¤:\n", cluster_detail['top_features'])
        print()
        print(f"Cluster {cluster_num}ìœ¼ë¡œ ë¶„ë¥˜ëœ ë¬¸ì„œë“¤:\n{cluster_detail['en_sent'][:10]}")
        print('-'*20)

feature_names = count.get_feature_names()


# In[169]:


cluster_details2 = get_cluster_details(cluster_model=kmeans,
                                     cluster_data=data_en2,
                                     feature_names=feature_names,
                                     cluster_num=5,
                                     top_n_features=5)


# In[170]:


cluster_details2[0]


# In[171]:


print_cluster_details(cluster_details2)


# In[172]:


a=" ".join(cluster_details2[0]['en_sent'])

wordcloud = WordCloud(font_path='font/NanumGothic.ttf', background_color='black', colormap='YlOrRd', relative_scaling=.5).generate(a) # generate() ëŠ” í•˜ë‚˜ì˜ string valueë¥¼ ì…ë ¥ ë°›ìŒ
plt.figure()
plt.imshow(wordcloud, interpolation='bilinear')
plt.axis("off")
plt.show()


# In[173]:


b=" ".join(cluster_details2[1]['en_sent'])

wordcloud = WordCloud(font_path='font/NanumGothic.ttf', background_color='black', colormap='YlOrRd', relative_scaling=.5).generate(b) # generate() ëŠ” í•˜ë‚˜ì˜ string valueë¥¼ ì…ë ¥ ë°›ìŒ
plt.figure()
plt.imshow(wordcloud, interpolation='bilinear')
plt.axis("off")
plt.show()


# In[174]:


c=" ".join(cluster_details2[2]['en_sent'])

wordcloud = WordCloud(font_path='font/NanumGothic.ttf', background_color='black', colormap='YlOrRd', relative_scaling=.5).generate(c) # generate() ëŠ” í•˜ë‚˜ì˜ string valueë¥¼ ì…ë ¥ ë°›ìŒ
plt.figure()
plt.imshow(wordcloud, interpolation='bilinear')
plt.axis("off")
plt.show()


# In[175]:


d=" ".join(cluster_details2[3]['en_sent'])

wordcloud = WordCloud(font_path='font/NanumGothic.ttf', background_color='black', colormap='YlOrRd', relative_scaling=.5).generate(d) # generate() ëŠ” í•˜ë‚˜ì˜ string valueë¥¼ ì…ë ¥ ë°›ìŒ
plt.figure()
plt.imshow(wordcloud, interpolation='bilinear')
plt.axis("off")
plt.show()


# In[176]:


e=" ".join(cluster_details2[4]['en_sent'])

wordcloud = WordCloud(font_path='font/NanumGothic.ttf', background_color='black', colormap='YlOrRd', relative_scaling=.5).generate(e) # generate() ëŠ” í•˜ë‚˜ì˜ string valueë¥¼ ì…ë ¥ ë°›ìŒ
plt.figure()
plt.imshow(wordcloud, interpolation='bilinear')
plt.axis("off")
plt.show()


# In[ ]:





# In[ ]:





# In[ ]:





# In[135]:


'''
data_en.head()

# í´ëŸ¬ìŠ¤í„°ë§ëœ ë¬¸ì„œë“¤ ì¤‘ì—ì„œ íŠ¹ì • ë¬¸ì„œë¥¼ í•˜ë‚˜ ì„ íƒí•œ í›„ ë¹„ìŠ·í•œ ë¬¸ì„œ ì¶”ì¶œ
from sklearn.metrics.pairwise import cosine_similarity

hotel_idx = data_en[data_en['label']==1].index
print("í˜¸í…” ì¹´í…Œê³ ë¦¬ë¡œ í´ëŸ¬ìŠ¤í„°ë§ëœ ë¬¸ì„œë“¤ì˜ ì¸ë±ìŠ¤:\n",hotel_idx)
print()
# í˜¸í…” ì¹´í…Œê³ ë¦¬ë¡œ í´ëŸ¬ìŠ¤í„°ë§ ëœ ë¬¸ì„œë“¤ì˜ ì¸ë±ìŠ¤ ì¤‘ í•˜ë‚˜ ì„ íƒí•´ ë¹„êµ ê¸°ì¤€ìœ¼ë¡œ ì‚¼ì„ ë¬¸ì„œ ì„ ì •
comparison_doc = data_en.iloc[hotel_idx[0]]['comment']
print("##ìœ ì‚¬ë„ ë¹„êµ ê¸°ì¤€ ë¬¸ì„œ ì´ë¦„:",comparison_doc,'##')
print()

# ìœ„ì—ì„œ ì¶”ì¶œí•œ í˜¸í…” ì¹´í…Œê³ ë¦¬ë¡œ í´ëŸ¬ìŠ¤í„°ë§ëœ ë¬¸ì„œë“¤ì˜ ì¸ë±ìŠ¤ ì¤‘ 0ë²ˆì¸ë±ìŠ¤(ë¹„êµê¸°ì¤€ë¬¸ì„œ)ì œì™¸í•œ
# ë‹¤ë¥¸ ë¬¸ì„œë“¤ê³¼ì˜ ìœ ì‚¬ë„ ì¸¡ì •
similarity = cosine_similarity(ftr_vect[hotel_idx[0]], ftr_vect[hotel_idx])
# print(similarity)
'''


# In[136]:


'''

# ë¹„êµê¸°ì¤€ ë¬¸ì„œì™€ ë‹¤ë¥¸ ë¬¸ì„œë“¤ê°„ì˜ ìœ ì‚¬ë„ ì‚´í´ë³´ê¸°
import seaborn as sns
import numpy as np
import matplotlib.pyplot as plt
# array ë‚´ë¦¼ì°¨ìˆœìœ¼ë¡œ ì •ë ¬í•œ í›„ ì¸ë±ìŠ¤ ë°˜í™˜ [:,::-1] ëª¨ë“ í–‰ì— ëŒ€í•´ì„œ ì—´ì„ ë‚´ë¦¼ì°¨ìˆœìœ¼ë¡œ!
sorted_idx = similarity.argsort()[:,::-1]
# ë¹„êµë¬¸ì„œ ë‹¹ì‚¬ìëŠ” ì œì™¸í•œ ì¸ë±ìŠ¤ ì¶”ì¶œ
sorted_idx = sorted_idx[:, 1:]

# ìœ ì‚¬ë„ê°€ í° ìˆœìœ¼ë¡œ hotel_idx(label=1ì¸ ì¦‰, í˜¸í…”ê³¼ê´€ë ¨ëœ ë‚´ìš©ì˜ ë¬¸ì„œì´ë¦„ë“¤ì˜ indexë“¤)ì—ì„œ ì¬ ì •ë ¬ 
# indexë¡œ ë„£ìœ¼ë ¤ë©´ 1ì°¨ì›ìœ¼ë¡œ reshapeí•´ì£¼ê¸°!
hotel_sorted_idx = hotel_idx[sorted_idx.reshape(-1,)]
# ìœ ì‚¬ë„ í–‰ë ¬ê°’ë“¤ì„ ìœ ì‚¬ë„ê°€ í° ìˆœìœ¼ë¡œ ì¬ì •ë ¬(ë¹„êµ ë¬¸ì„œ ë‹¹ì‚¬ìëŠ” ì œì™¸)
hotel_sim_values = np.sort(similarity.reshape(-1,))[::-1]
hotel_sim_values = hotel_sim_values[1:]
# ì´ë ‡ê²Œ ë˜ë©´ ë¹„êµë¬¸ì„œì™€ ê°€ì¥ ìœ ì‚¬í•œ ìˆœìœ¼ë¡œ 'í•´ë‹¹ë¬¸ì„œì˜index-ìœ ì‚¬ë„ê°’' ìœ¼ë¡œ ë™ì¼í•œ ìœ„ì¹˜ê°€ ë§¤í•‘ëœ ë‘ ê°œì˜ array!
# ê·¸ë˜ì„œ ê·¸ëŒ€ë¡œ ë°ì´í„°í”„ë ˆì„ì˜ ê° ì¹¼ëŸ¼ìœ¼ë¡œ ë„£ì–´ì£¼ê¸°
# print(hotel_sorted_idx)
# print(hotel_sim_values)
print()
print("ê¸¸ì´ ë¹„êµ", len(hotel_sorted_idx), len(hotel_sim_values))
print()
# ë¹ˆ ë°ì´í„°í”„ë ˆì„ ìƒì„±
hotel_sim_df = pd.DataFrame()
# hotel_sorted_idx ì™€ hotel_sim_values ë§¤í•‘ì‹œí‚¨ arrayì„
hotel_sim_df['comment'] = data_en.iloc[hotel_sorted_idx]['comment']
hotel_sim_df['similarity'] = hotel_sim_values

plt.figure(figsize=(15,10))
sns.barplot(data=hotel_sim_df[:10], x='similarity', y='comment')
plt.title(comparison_doc)


# In[62]:


from collections import Counter

def scan_vocabulary(sents, tokenize, min_count=2):
    counter = Counter(w for sent in sents for w in tokenize(sent))
    counter = {w:c for w,c in counter.items() if c >= min_count}
    idx_to_vocab = [w for w, _ in sorted(counter.items(), key=lambda x:-x[1])]
    vocab_to_idx = {vocab:idx for idx, vocab in enumerate(idx_to_vocab)}
    return idx_to_vocab, vocab_to_idx
    
from collections import defaultdict

def cooccurrence(tokens, vocab_to_idx, window=2, min_cooccurrence=2):
    counter = defaultdict(int)
    for s, tokens_i in enumerate(tokens):
        vocabs = [vocab_to_idx[w] for w in tokens_i if w in vocab_to_idx]
        n = len(vocabs)
        for i, v in enumerate(vocabs):
            if window <= 0:
                b, e = 0, n
            else:
                b = max(0, i - window)
                e = min(i + window, n)
            for j in range(b, e):
                if i == j:
                    continue
                counter[(v, vocabs[j])] += 1
                counter[(vocabs[j], v)] += 1
    counter = {k:v for k,v in counter.items() if v >= min_cooccurrence}
    n_vocabs = len(vocab_to_idx)
    return dict_to_mat(counter, n_vocabs, n_vocabs)
from scipy.sparse import csr_matrix

def dict_to_mat(d, n_rows, n_cols):
    rows, cols, data = [], [], []
    for (i, j), v in d.items():
        rows.append(i)
        cols.append(j)
        data.append(v)
    return csr_matrix((data, (rows, cols)), shape=(n_rows, n_cols))
def word_graph(sents, tokenize=None, min_count=2, window=2, min_cooccurrence=2):
    idx_to_vocab, vocab_to_idx = scan_vocabulary(sents, tokenize, min_count)
    tokens = [tokenize(sent) for sent in sents]
    g = cooccurrence(tokens, vocab_to_idx, window, min_cooccurrence, verbose)
    return g, idx_to_vocab
import numpy as np
from sklearn.preprocessing import normalize

def pagerank(x, df=0.85, max_iter=30):
    assert 0 < df < 1

    # initialize
    A = normalize(x, axis=0, norm='l1')
    R = np.ones(A.shape[0]).reshape(-1,1)
    bias = (1 - df) * np.ones(A.shape[0]).reshape(-1,1)

    # iteration
    for _ in range(max_iter):
        R = df * (A * R) + bias

    return R
def textrank_keyword(sents, tokenize, min_count, window, min_cooccurrence, df=0.85, max_iter=30, topk=30):
    g, idx_to_vocab = word_graph(sents, tokenize, min_count, window, min_cooccurrence)
    R = pagerank(g, df, max_iter).reshape(-1)
    idxs = R.argsort()[-topk:]
    keywords = [(idx_to_vocab[idx], R[idx]) for idx in reversed(idxs)]
    return keywords

from collections import Counter
from scipy.sparse import csr_matrix
import math

def sent_graph(sents, tokenize, similarity, min_count=2, min_sim=0.3):
    _, vocab_to_idx = scan_vocabulary(sents, tokenize, min_count)

    tokens = [[w for w in tokenize(sent) if w in vocab_to_idx] for sent in sents]
    rows, cols, data = [], [], []
    n_sents = len(tokens)
    for i, tokens_i in enumerate(tokens):
        for j, tokens_j in enumerate(tokens):
            if i >= j:
                continue
            sim = similarity(tokens_i, tokens_j)
            if sim < min_sim:
                continue
            rows.append(i)
            cols.append(j)
            data.append(sim)
    return csr_matrix((data, (rows, cols)), shape=(n_sents, n_sents))

def textrank_sent_sim(s1, s2):
    n1 = len(s1)
    n2 = len(s2)
    if (n1 <= 1) or (n2 <= 1):
        return 0
    common = len(set(s1).intersection(set(s2)))
    base = math.log(n1) + math.log(n2)
    return common / base

def cosine_sent_sim(s1, s2):
    if (not s1) or (not s2):
        return 0

    s1 = Counter(s1)
    s2 = Counter(s2)
    norm1 = math.sqrt(sum(v ** 2 for v in s1.values()))
    norm2 = math.sqrt(sum(v ** 2 for v in s2.values()))
    prod = 0
    for k, v in s1.items():
        prod += v * s2.get(k, 0)
    return prod / (norm1 * norm2)
def textrank_keysentence(sents, tokenize, min_count, similarity, df=0.85, max_iter=30, topk=5):
    g = sent_graph(sents, tokenize, min_count, min_sim, similarity)
    R = pagerank(g, df, max_iter).reshape(-1)
    idxs = R.argsort()[-topk:]
    keysents = [(idx, R[idx], sents[idx]) for idx in reversed(idxs)]
    return keysents
'''


# In[ ]:




