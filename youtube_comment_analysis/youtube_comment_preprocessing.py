# -*- coding: utf-8 -*-
"""youtube_comment_preprocessing

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1jx9QTbAwKqWeS2sN7fgcHhNxEWblF11F
"""

"""# 언어별 분류 작업
- 정확도가 높은 fasttext 모듈로 분류
"""
!pip install fasttext
import copy
import fasttext
import pandas as pd
import re

def preprocessing(name):
        
    path = '/content/drive/MyDrive/Colab Notebooks/[공유] Mulcam_Army 공유폴더!/크롤링 한 자료/youtube/영상별 댓글/'
    comment_file = f'comments_youtube_{name}.csv'     #aespa
    df = pd.read_csv(path+comment_file, encoding='utf-8', header=None)
    df.columns=['comment','like']
    
    # 중복 값 제거
    print(f"{name} 전처리 시작")
    print('\n')
    print('중복 제거 전 :',df.shape)
    df = df.drop_duplicates(['comment'],keep='last',ignore_index=True)
    print(f'{name} 중복 제거 후 :',df.shape)
    print('\n')

    # 소문자로 바꾸기
    df['comment'] = df['comment'].str.lower()
    
    # 전처리 전 원본 보존
    copy_data = copy.deepcopy(df)
    
    emoji_pattern = re.compile("["
            u"\U0001F600-\U0001F64F"  # emoticons
            u"\U0001F300-\U0001F5FF"  # symbols & pictographs
            u"\U0001F680-\U0001F6FF"  # transport & map symbols
            u"\U0001F1E0-\U0001F1FF"  # flags (iOS)
                            "]+", flags=re.UNICODE)

    #분석에 어긋나는 불용어구 제외 (특수문자, 의성어)
    han = re.compile(r'[ㄱ-ㅎㅏ-ㅣ!?~,".\n\r#\ufeff\u200d]')

    comment_result = []

    for i in copy_data['comment'].values:
        tokens = re.sub(emoji_pattern,"",i)
        tokens = re.sub(han,"",tokens)
        comment_result.append(tokens)

    punct = "/-'?!.,#$%\'()*+-/:;<=>@[\\]^_`{|}~" + '""“”’' + '∞θ÷α•à−β∅³π‘₹´°£€\×™√²—–&'

    punct_mapping = {"‘": "'", "₹": "e", "´": "'", "°": "", "€": "e", "™": "tm", "√": " sqrt ", "×": "x", "²": "2", "—": "-", "–": "-", "’": "'", "_": "-", "`": "'", '“': '"', '”': '"', '“': '"', "£": "e", '∞': 'infinity', 'θ': 'theta', '÷': '/', 'α': 'alpha', '•': '.', 'à': 'a', '−': '-', 'β': 'beta', '∅': '', '³': '3', 'π': 'pi', }

    def clean_punc(text, punct, mapping):
        for p in mapping:
            text = text.replace(p, mapping[p])
        
        for p in punct:
            text = text.replace(p, f' {p} ')
        
        specials = {'\u200b': ' ', '…': ' ... ', '\ufeff': '', 'करना': '', 'है': ''}
        for s in specials:
            text = text.replace(s, specials[s])
        
        return text.strip()

    cleaned_corpus = []
    for sent in comment_result:
        cleaned_corpus.append(clean_punc(sent, punct, punct_mapping))



    def clean_text(texts):
        corpus = []
        for i in range(0, len(texts)):
            review = re.sub(r'[@%\\*=()/~#&\+á?\xc3\xa1\-\|\.\:\;\!\-\,\_\~\$\'\"]', '',str(texts[i])) #remove punctuation
            review = re.sub(r'\d+','', str(texts[i]))# remove number
            review = review.lower() #lower case
            review = re.sub(r'\s+', ' ', review) #remove extra space
            review = re.sub(r'<[^>]+>','',review) #remove Html tags
            review = re.sub(r'\s+', ' ', review) #remove spaces
            review = re.sub(r"^\s+", '', review) #remove space from start
            review = re.sub(r'\s+$', '', review) #remove space from the end
            corpus.append(review)
        return corpus

    basic_preprocessed_corpus = clean_text(cleaned_corpus)
    comment_result = pd.DataFrame(basic_preprocessed_corpus, columns=["comment"])

    model = fasttext.load_model('/content/drive/MyDrive/Colab Notebooks/[공유] Mulcam_Army 공유폴더!/lid.176.ftz')

    predict = []
    for t in comment_result.comment.values:
        predict.append(model.predict(t,k=1))

    ty = pd.DataFrame(predict)

    comment = []
    for num, txt in enumerate(ty[0]):
        txt = str(txt)
        if txt == "('__label__ko',)":
            b = re.sub(txt,"ko",txt)
            comment.append(b)
        elif txt == "('__label__en',)":
            b = re.sub(txt,"en",txt)
            comment.append(b)
        elif txt == "('__label__id',)":
            b = re.sub(txt,"id",txt)
            comment.append(b)
        elif txt == "('__label__es',)":
            b = re.sub(txt,"es",txt)
            comment.append(b)
        else:
            b = re.sub(txt,"etc",txt)
            comment.append(b)

    comment = pd.DataFrame(comment)
    print('\n')
    print(f'{name} 댓글 언어 구성')
    print(comment.value_counts())
    print('\n')
    pd.set_option('max_columns',50)
    pd.set_option('max_rows',100)
    # ty_sum.to_csv('ty_sum.csv', encoding='cp949')


    like = pd.DataFrame(copy_data['like'])
    data = pd.concat([comment_result,like, comment],axis=1)
    data.columns = ['comment','like','lang']


    path_preprocess='/content/drive/MyDrive/Colab Notebooks/[공유] Mulcam_Army 공유폴더!/크롤링 한 자료/youtube/preprocesing_comment/'
    data.to_csv(path_preprocess+'prepro_'+comment_file,  encoding='utf-8', header=None)

    print(f"{name} 전처리 끝")
    print('\n')
    return data

artist_name=pd.read_csv('/content/drive/MyDrive/Colab Notebooks/[공유] Mulcam_Army 공유폴더!/아티스트이름 - Sheet1.csv')
name=artist_name['engName']
name.dropna(inplace=True)
name=name.reset_index(drop=True)
name.drop([3,5,24,29,37,38,48],axis=0,inplace=True)
name=name.reset_index(drop=True)
name[:4]

for i in range(len(name)):
    preprocessing(name[i])

